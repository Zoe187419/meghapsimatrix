<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>propensity scores | Megha</title>
    <link>/tags/propensity-scores/</link>
      <atom:link href="/tags/propensity-scores/index.xml" rel="self" type="application/rss+xml" />
    <description>propensity scores</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Tue, 23 Jun 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/img/icon-192.png</url>
      <title>propensity scores</title>
      <link>/tags/propensity-scores/</link>
    </image>
    
    <item>
      <title>Propensity Score Analysis with Multiply Imputed Data</title>
      <link>/post/mi_ps/</link>
      <pubDate>Tue, 23 Jun 2020 00:00:00 +0000</pubDate>
      <guid>/post/mi_ps/</guid>
      <description>


&lt;p&gt;Work in progress…&lt;/p&gt;
&lt;p&gt;In this post, I walk through steps of running propensity scores when there is missingness in the covariate data. Particularly, I look at multiple imputation and ways to condition on propensity scores estimated on imputed data. The code builds on my earlier &lt;a href=&#34;https://meghapsimatrix.com/post/missing_dat/&#34;&gt;post&lt;/a&gt; that goes over different ways to handle missing data when conducting propensity score analysis.&lt;/p&gt;
&lt;p&gt;When using multiple imputations, &lt;span class=&#34;citation&#34;&gt;Hill (2004)&lt;/span&gt; and &lt;span class=&#34;citation&#34;&gt;Mitra and Reiter (2016)&lt;/span&gt; examined two distinct ways to condition on the propensity scores:&lt;/p&gt;
&lt;div id=&#34;multiple-imputation-across-mi-across&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Multiple Imputation Across (MI Across)&lt;/h2&gt;
&lt;p&gt;This approach involves creating &lt;em&gt;m&lt;/em&gt; imputed datasets and then estimating propensity scores within each of the datasets and then averaging each unit’s &lt;em&gt;m&lt;/em&gt; propensity scores across the &lt;em&gt;m&lt;/em&gt; datasets &lt;span class=&#34;citation&#34;&gt;(Hill 2004)&lt;/span&gt;. Stratification, matching or IPW can be implemented using these averaged propensity scores &lt;span class=&#34;citation&#34;&gt;(Hill 2004)&lt;/span&gt;. Outcome models that include covariates will need to use the weights or strata derived from the averaged propensity scores and the &lt;em&gt;m&lt;/em&gt; sets of covariate values. The weighted regression estimates will then need to be pooled.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;multiple-imputation-within-mi-within&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Multiple Imputation Within (MI Within)&lt;/h2&gt;
&lt;p&gt;This approach involves creating &lt;em&gt;m&lt;/em&gt; imputed datasets and then estimating propensity scores within each of the datasets &lt;span class=&#34;citation&#34;&gt;(Hill 2004)&lt;/span&gt;. Instead of averaging the propensity scores across the datasets, this method entails conditioning on the propensity scores within the datasets and running the outcome analyses within each dataset &lt;span class=&#34;citation&#34;&gt;(Hill 2004)&lt;/span&gt;. The separate regression estimates have to be pooled.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;read-in-the-data&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Read in the data&lt;/h1&gt;
&lt;p&gt;Below, I show how to implement the Across and Within methods to estimate the ATT. The data that I use here is from High School and Beyond (HSB) longitudinal study used by &lt;span class=&#34;citation&#34;&gt;Rosenbaum (1986)&lt;/span&gt; to analyze the effect of dropping out of high school on later math achievement. For the purpose of this example, I am going to assume a simple random sample.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(mice)
library(twang)
library(estimatr)
library(broom)
library(naniar)
library(knitr)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;hsls_dat &amp;lt;- read_csv(&amp;quot;https://raw.githubusercontent.com/meghapsimatrix/datasets/master/causal/HSLS09_incomplete.csv&amp;quot;) %&amp;gt;%
  mutate_if(is.character, as.factor) %&amp;gt;%
  mutate_at(vars(repeated_grade, IEP), as.factor) %&amp;gt;%
  select(-working_T3) # an outcome we don&amp;#39;t care about&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Below, using &lt;code&gt;gg_miss_var&lt;/code&gt; from the &lt;code&gt;naniar&lt;/code&gt; package, I visualize the percent of data that are missing for each of the variables &lt;span class=&#34;citation&#34;&gt;(Tierney et al. 2020)&lt;/span&gt;. The treatment variable, &lt;code&gt;drop_status&lt;/code&gt; has no missing data. The outcome variable, &lt;code&gt;math_score_T2&lt;/code&gt;, however, does have around 10% missing data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gg_miss_var(hsls_dat, show_pct = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/mi_ps_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;multiple-imputation-using-mice&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Multiple imputation using &lt;code&gt;mice&lt;/code&gt;&lt;/h1&gt;
&lt;p&gt;Here, I impute the data using the &lt;code&gt;mice&lt;/code&gt; package and set the number of imputations and itereations to 10 each and also set the seed &lt;span class=&#34;citation&#34;&gt;(van Buuren and Groothuis-Oudshoorn 2011)&lt;/span&gt;. Then, I save the result as an &lt;code&gt;RData&lt;/code&gt; file. For imputation method, I let mice run the default methods: predicitive mean matching for continuous variables, Bayesian logistic regression for binary variables and Bayesian polytomous regression for multinomial variables. For more information on methods for imputation, please see &lt;a href=&#34;https://stefvanbuuren.name/fimd/sec-modelform.html&#34;&gt;Chapter 6.2&lt;/a&gt; from &lt;a href=&#34;https://stefvanbuuren.name/fimd/&#34;&gt;Flexible Modeling of Missing Data&lt;/a&gt; &lt;span class=&#34;citation&#34;&gt;(van Buuren 2018)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;system.time(temp_data &amp;lt;- mice(hsls_dat, m = 10, maxit = 10, seed = 20200516))

save(temp_data, file = &amp;quot;temp_data.RData&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;long-format&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Long format&lt;/h2&gt;
&lt;p&gt;I then load the saved RData file and then extract a long format data that contains each of the 10 imputed data stacked.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;load(&amp;quot;temp_data.RData&amp;quot;)
imp_dat &amp;lt;- complete(temp_data, action = &amp;quot;long&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;estimating-propensity-scores&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Estimating propensity scores&lt;/h1&gt;
&lt;p&gt;Below I create a dataset with only the covariates and use the &lt;code&gt;paste()&lt;/code&gt; function to create propensity score model equation. For the sake of this example, I only focus on including main effects of the covariates in the propensity score model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;covs &amp;lt;- imp_dat %&amp;gt;%
  select(sex:climate, math_score_T1)

equation_ps &amp;lt;- paste(&amp;quot;drop_status ~ &amp;quot;, paste(names(covs), collapse = &amp;quot; + &amp;quot;))
equation_ps&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;drop_status ~  sex + race + language + repeated_grade + IEP + locale + region + SES + math_identity + math_utility + math_efficacy + math_interest + engagement + belonging + expectations + climate + math_score_T1&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here, I create a function &lt;code&gt;estimate_ps()&lt;/code&gt; that takes in an equation and a dataset, then runs logistic regression using &lt;code&gt;glm()&lt;/code&gt;. Then the function adds logit of propensity scores and propensity scores as columns in the dataset.&lt;/p&gt;
&lt;p&gt;I then group the &lt;code&gt;imp_dat&lt;/code&gt; by the imputation number and then run the &lt;code&gt;estimate_ps()&lt;/code&gt; function on each of the imputed dataset using the &lt;code&gt;do()&lt;/code&gt; function from &lt;code&gt;dplyr&lt;/code&gt; &lt;span class=&#34;citation&#34;&gt;(Wickham et al. 2019)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;estimate_ps &amp;lt;- function(equation, dat){
  
  ps_model &amp;lt;- glm(as.formula(equation), family = binomial, data = dat)
  
  dat &amp;lt;- dat %&amp;gt;%
    mutate(ps_logit = predict(ps_model, type = &amp;quot;link&amp;quot;),
           ps = predict(ps_model, type = &amp;quot;response&amp;quot;))
  
  return(dat)
  
}


imp_dat_ps &amp;lt;- imp_dat %&amp;gt;%
  group_by(.imp) %&amp;gt;%
  do(estimate_ps(equation_ps, .)) %&amp;gt;%
  ungroup()&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;estimating-att-ipw-weights&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Estimating ATT IPW weights&lt;/h1&gt;
&lt;p&gt;Below I estimate ATT weights using the across and within methods. For the Across method, I use the average of the propensity scores across the imputed datasets to calculate weights. For the within method, I use the the propensity scores estimated within each imputed dataset to calculate weights.&lt;/p&gt;
&lt;p&gt;The code below groups the imputed data by &lt;code&gt;.id&lt;/code&gt; which is an identifier denoting each case. For each case, I summarize the mean of the propensity scores across the 10 imputed dataset. I then estimate the ATT weights using the averaged propensity scores for the Across method and the original propensity scores for the Within method.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;imp_dat_ps &amp;lt;- imp_dat_ps %&amp;gt;%
  group_by(.id) %&amp;gt;%
  mutate(ps_across = mean(ps)) %&amp;gt;%
  ungroup() %&amp;gt;%
  mutate(att_wt_across = drop_status + (1 - drop_status) * ps_across/(1 - ps_across),
         att_wt_within = drop_status + (1 - drop_status) * ps/(1 - ps))


imp_dat_ps %&amp;gt;%
  select(.imp, ps, ps_across, att_wt_across, att_wt_within)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 214,020 x 5
##     .imp       ps ps_across att_wt_across att_wt_within
##    &amp;lt;int&amp;gt;    &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;         &amp;lt;dbl&amp;gt;         &amp;lt;dbl&amp;gt;
##  1     1 0.00339   0.00330       0.00331       0.00340 
##  2     1 0.00989   0.0102        0.0103        0.00999 
##  3     1 0.00135   0.00131       0.00132       0.00135 
##  4     1 0.0119    0.0114        0.0115        0.0120  
##  5     1 0.00222   0.00246       0.00246       0.00222 
##  6     1 0.00289   0.00294       0.00295       0.00290 
##  7     1 0.0113    0.0101        0.0102        0.0114  
##  8     1 0.00347   0.00375       0.00377       0.00348 
##  9     1 0.00327   0.00276       0.00276       0.00328 
## 10     1 0.000667  0.000633      0.000633      0.000667
## # … with 214,010 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;checking-balance&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Checking balance&lt;/h1&gt;
&lt;/div&gt;
&lt;div id=&#34;estimating-att&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Estimating ATT&lt;/h1&gt;
&lt;p&gt;Below I create a function to estimate the ATT. The function takes in an equation, a dataset, and weights as arguments. Then it runs a model using &lt;code&gt;lm_robust()&lt;/code&gt; from the &lt;code&gt;estimatr&lt;/code&gt; package &lt;span class=&#34;citation&#34;&gt;(Blair et al. 2020)&lt;/span&gt;. The standard errors of the regression coefficients are estimated using &lt;code&gt;HC2&lt;/code&gt; type sandwich errors. I then clean up the results using &lt;code&gt;tidy()&lt;/code&gt; from &lt;code&gt;broom&lt;/code&gt; &lt;span class=&#34;citation&#34;&gt;(Robinson and Hayes 2020)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;estimate_ATT &amp;lt;- function(equation, dat, wts){
  
  wts &amp;lt;- dat %&amp;gt;% pull({{wts}})
  
  model &amp;lt;- lm_robust(as.formula(equation), data = dat, weights = wts)
  
  res &amp;lt;- model %&amp;gt;%
    tidy() %&amp;gt;%
    filter(term == &amp;quot;drop_status&amp;quot;) %&amp;gt;%
    select(term, estimate, se = std.error, dci_low = conf.low, ci_high = conf.high, df = df)
  
  return(res)
  
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I set up an equation regressing the outcome variable, &lt;code&gt;math_score_T2&lt;/code&gt; on drop status and on the main effects of all the covariates. I then run the &lt;code&gt;estimate_ATT()&lt;/code&gt; function on each imputed data using &lt;code&gt;group_by()&lt;/code&gt; and &lt;code&gt;do()&lt;/code&gt;. The weights are different for the Across and Within methods.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;equation_ancova &amp;lt;- paste(&amp;quot;math_score_T2 ~ drop_status + &amp;quot;, paste(names(covs), collapse = &amp;quot; + &amp;quot;))
equation_ancova&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;math_score_T2 ~ drop_status +  sex + race + language + repeated_grade + IEP + locale + region + SES + math_identity + math_utility + math_efficacy + math_interest + engagement + belonging + expectations + climate + math_score_T1&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;across_res &amp;lt;- imp_dat_ps %&amp;gt;%
  group_by(.imp) %&amp;gt;%
  do(estimate_ATT(equation = equation_ancova, dat = ., wts = att_wt_across)) %&amp;gt;%
  ungroup()

within_res &amp;lt;- imp_dat_ps %&amp;gt;%
  group_by(.imp) %&amp;gt;%
  do(estimate_ATT(equation = equation_ancova, dat = ., wts = att_wt_within)) %&amp;gt;%
  ungroup()&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;pooling-the-results&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Pooling the results&lt;/h1&gt;
&lt;p&gt;Here, I create a function called &lt;code&gt;calc_pooled()&lt;/code&gt; using formula by &lt;span class=&#34;citation&#34;&gt;Barnard and Rubin (1999)&lt;/span&gt; to pool the results across the imputations. The &lt;code&gt;mice&lt;/code&gt; package has the &lt;code&gt;pool()&lt;/code&gt; function to do the same thing but we would need to convert the imputed data back to &lt;code&gt;mids&lt;/code&gt; object type and I just wanted to skip all that :D&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;calc_pooled &amp;lt;- function(dat, est, se, df){
  
  dat &amp;lt;- dat %&amp;gt;%
    mutate(est = dat %&amp;gt;% pull({{est}}),
           se = dat %&amp;gt;%pull({{se}}),
           df = dat %&amp;gt;% pull({{df}}))
  
  pooled &amp;lt;- dat %&amp;gt;%
    summarize(m = n(),
              B = var(est),  # between imputation var
              beta_bar = mean(est), # mean of estimated reg coeffs
              V_bar = mean(se^2), # mean of var - hc corrected   within imp var
              eta_bar = mean(df)) %&amp;gt;%   # mean of df
    mutate(
      
      V_total = V_bar + B * (m + 1) / m,  #between and within var est
      gamma = ((m + 1) / m) * B / V_total,  
      df_m = (m - 1) / gamma^2,
      df_obs = eta_bar * (eta_bar + 1) * (1 - gamma) / (eta_bar + 3),
      df = 1 / (1 / df_m + 1 / df_obs),
      
      # output
      se = sqrt(V_total),
      ci_lower = beta_bar - se * qt(0.975, df = df),
      ci_upper = beta_bar + se * qt(0.975, df = df)) %&amp;gt;%
    
    select(est = beta_bar, se, df, ci_lower, ci_upper) 
  
  return(pooled)
  
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Below I use the &lt;code&gt;calc_pooled()&lt;/code&gt; function to pool the results for each of the methods.&lt;/p&gt;
&lt;div id=&#34;across&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Across&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;across_pooled &amp;lt;- calc_pooled(dat = across_res, est = estimate, se = se, df = df)
across_pooled %&amp;gt;%
  kable(digits = 3)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;right&#34;&gt;est&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;se&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;df&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;ci_lower&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;ci_upper&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;-0.335&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.038&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;93.772&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.411&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.26&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;within&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Within&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;within_pooled &amp;lt;- calc_pooled(dat = within_res, est = estimate, se = se, df = df)
within_pooled %&amp;gt;%
  kable(digits = 3)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;right&#34;&gt;est&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;se&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;df&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;ci_lower&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;ci_upper&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;-0.356&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.041&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;50.002&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.438&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.273&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;comparing-the-methods&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Comparing the methods&lt;/h1&gt;
&lt;p&gt;The estimates, se and df are different for the results from the two different methods. So which one is better? It’s not possible to say based on analysis on one dataset. &lt;span class=&#34;citation&#34;&gt;Hill (2004)&lt;/span&gt; and &lt;span class=&#34;citation&#34;&gt;Mitra and Reiter (2016)&lt;/span&gt; conducted simulation studies comparing the two methods. &lt;span class=&#34;citation&#34;&gt;Hill (2004)&lt;/span&gt; found that MI Across performed best in terms of absolute bias and mean squared error compared to all the other methods examined in the study. &lt;span class=&#34;citation&#34;&gt;Mitra and Reiter (2016)&lt;/span&gt; found that MI Across resulted in greater bias reduction in the estimation of ATT compared to MI Within. For details of the simulation studies please see the articles.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1 unnumbered&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;div id=&#34;refs&#34; class=&#34;references&#34;&gt;
&lt;div id=&#34;ref-barnard_small-sample_nodate&#34;&gt;
&lt;p&gt;Barnard, John, and Donald B Rubin. 1999. “Small-Sample Degrees of Freedom with Multiple Imputation,” 9.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-estimatr_2020&#34;&gt;
&lt;p&gt;Blair, Graeme, Jasper Cooper, Alexander Coppock, Macartan Humphreys, and Luke Sonnet. 2020. &lt;em&gt;Estimatr: Fast Estimators for Design-Based Inference&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=estimatr&#34;&gt;https://CRAN.R-project.org/package=estimatr&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-hill_2004&#34;&gt;
&lt;p&gt;Hill, Jennifer. 2004. “Reducing Bias in Treatment Effect Estimation in Observational Studies Suffering from Missing Data.” Columbia University Institute for Social &amp;amp; Economic Research &amp;amp; Policy (ISERP).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-mitra_comparison_2016&#34;&gt;
&lt;p&gt;Mitra, Robin, and Jerome P Reiter. 2016. “A Comparison of Two Methods of Estimating Propensity Scores After Multiple Imputation.” &lt;em&gt;Statistical Methods in Medical Research&lt;/em&gt; 25 (1): 188–204. &lt;a href=&#34;https://doi.org/10.1177/0962280212445945&#34;&gt;https://doi.org/10.1177/0962280212445945&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-broom_2020&#34;&gt;
&lt;p&gt;Robinson, David, and Alex Hayes. 2020. &lt;em&gt;Broom: Convert Statistical Analysis Objects into Tidy Tibbles&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=broom&#34;&gt;https://CRAN.R-project.org/package=broom&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-rosenbaum1986dropping&#34;&gt;
&lt;p&gt;Rosenbaum, Paul R. 1986. “Dropping Out of High School in the United States: An Observational Study.” &lt;em&gt;Journal of Educational Statistics&lt;/em&gt; 11 (3). Sage Publications Sage CA: Los Angeles, CA: 207–24.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-naniar_2020&#34;&gt;
&lt;p&gt;Tierney, Nicholas, Di Cook, Miles McBain, and Colin Fay. 2020. &lt;em&gt;Naniar: Data Structures, Summaries, and Visualisations for Missing Data&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=naniar&#34;&gt;https://CRAN.R-project.org/package=naniar&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-van2018flexible&#34;&gt;
&lt;p&gt;van Buuren, Stef. 2018. &lt;em&gt;Flexible Imputation of Missing Data&lt;/em&gt;. Chapman; Hall/CRC.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-vanB_2011&#34;&gt;
&lt;p&gt;van Buuren, Stef, and Karin Groothuis-Oudshoorn. 2011. “mice: Multivariate Imputation by Chained Equations in R.” &lt;em&gt;Journal of Statistical Software&lt;/em&gt; 45 (3): 1–67. &lt;a href=&#34;http://www.jstatsoft.org/v45/i03/&#34;&gt;http://www.jstatsoft.org/v45/i03/&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-tidy_2020&#34;&gt;
&lt;p&gt;Wickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy D’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019. “Welcome to the tidyverse.” &lt;em&gt;Journal of Open Source Software&lt;/em&gt; 4 (43): 1686. &lt;a href=&#34;https://doi.org/10.21105/joss.01686&#34;&gt;https://doi.org/10.21105/joss.01686&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
