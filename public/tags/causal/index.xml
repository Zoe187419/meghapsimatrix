<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>causal | Megha</title>
    <link>/tags/causal/</link>
      <atom:link href="/tags/causal/index.xml" rel="self" type="application/rss+xml" />
    <description>causal</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Tue, 23 Jun 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/img/icon-192.png</url>
      <title>causal</title>
      <link>/tags/causal/</link>
    </image>
    
    <item>
      <title>Propensity Scores Analysis with Multiply Imputed Data</title>
      <link>/post/mi_ps/</link>
      <pubDate>Tue, 23 Jun 2020 00:00:00 +0000</pubDate>
      <guid>/post/mi_ps/</guid>
      <description>


&lt;p&gt;Work in progress…&lt;/p&gt;
&lt;p&gt;In this post, I walk through steps of running propensity scores when there is missingness in the covariate data. Particularly, I look at multiple imputation and ways to condition on propensity scores estimated on imputed data. The code builds on my earlier &lt;a href=&#34;https://meghapsimatrix.com/post/missing_dat/&#34;&gt;post&lt;/a&gt; that goes over different ways to handle missing data when conducting propensity score analysis.&lt;/p&gt;
&lt;p&gt;When using multiple imputations, Hill (2004) and Mitra and Reiter (2016) examined two distinct ways to condition on the propensity scores:&lt;/p&gt;
&lt;div id=&#34;multiple-imputation-across-mi-across&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Multiple Imputation Across (MI Across)&lt;/h2&gt;
&lt;p&gt;This approach involves creating &lt;em&gt;m&lt;/em&gt; imputed datasets and then estimating propensity scores within each of the datasets and then averaging each unit’s &lt;em&gt;m&lt;/em&gt; propensity scores across the &lt;em&gt;m&lt;/em&gt; datasets &lt;span class=&#34;citation&#34;&gt;[@hill_2004]&lt;/span&gt;. Stratification, matching or IPW can be implemented using these averaged propensity scores &lt;span class=&#34;citation&#34;&gt;[@hill_2004]&lt;/span&gt;. Outcome models that include covariates will need to use the weights or strata derived from the averaged propensity scores and the &lt;em&gt;m&lt;/em&gt; sets of covariate values. The weighted regression estimates will then need to be pooled.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;multiple-imputation-within-mi-within&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Multiple Imputation Within (MI Within)&lt;/h2&gt;
&lt;p&gt;This approach involves creating &lt;em&gt;m&lt;/em&gt; imputed datasets and then estimating propensity scores within each of the datasets &lt;span class=&#34;citation&#34;&gt;[@hill_2004]&lt;/span&gt;. Instead of averaging the propensity scores across the datasets, this method entails conditioning on the propensity scores within the datasets and running the outcome analyses within each dataset &lt;span class=&#34;citation&#34;&gt;[@hill_2004]&lt;/span&gt;. The separate regression estimates have to be pooled.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;read-in-the-data&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Read in the data&lt;/h1&gt;
&lt;p&gt;Below, I show how to implement the Across and Within methods to estimate the ATT. The data that I use here is from High School and Beyond (HSB) longitudinal study used by Rosenbaum (1986) to analyze the effect of dropping out of high school on later math achievement.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(mice)
library(twang)
library(estimatr)
library(broom)
library(naniar)
library(knitr)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;hsls_dat &amp;lt;- read_csv(&amp;quot;https://raw.githubusercontent.com/meghapsimatrix/datasets/master/causal/HSLS09_incomplete.csv&amp;quot;) %&amp;gt;%
  mutate_if(is.character, as.factor) %&amp;gt;%
  mutate_at(vars(repeated_grade, IEP), as.factor) %&amp;gt;%
  select(-working_T3) # an outcome we don&amp;#39;t care about&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Below, using &lt;code&gt;gg_miss_var&lt;/code&gt; from the &lt;code&gt;naniar&lt;/code&gt; package, I visualize the percent of data that are missing for each of the variables. The treatment variable, &lt;code&gt;drop_status&lt;/code&gt; has no missing data. The outcome variable, &lt;code&gt;math_score_T2&lt;/code&gt;, however, does have around 10% missing data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gg_miss_var(hsls_dat, show_pct = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/mi_ps_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;multiple-imputation-using-mice&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Multiple imputation using &lt;code&gt;mice&lt;/code&gt;&lt;/h1&gt;
&lt;p&gt;Here, I impute the data using mice and set the number of imputations and itereations to 10 each and also set the seed. Then, I save the result as an &lt;code&gt;RData&lt;/code&gt; file. For imputation method, I let mice run the default methods: predicitive mean matching for continuous variables, Bayesian logistic regression for binary variables and Bayesian polytomous regression for multinomial variables. For more information on methods for imputation, please see &lt;a href=&#34;https://stefvanbuuren.name/fimd/sec-modelform.html&#34;&gt;Chapter 6.2&lt;/a&gt; from &lt;a href=&#34;https://stefvanbuuren.name/fimd/&#34;&gt;Flexible Modeling of Missing Data&lt;/a&gt;, van Buuren (2018).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;system.time(temp_data &amp;lt;- mice(hsls_dat, m = 10, maxit = 10, seed = 20200516))

save(temp_data, file = &amp;quot;temp_data.RData&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;long-format-and-propensity-score-model&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Long format and propensity score model&lt;/h1&gt;
&lt;p&gt;I then load the saved RData file and then extract a long format data that contains each of the 10 imputed data stacked.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;load(&amp;quot;temp_data.RData&amp;quot;)
imp_dat &amp;lt;- complete(temp_data, action = &amp;quot;long&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;function-to-estimate-propensity-scores&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Function to estimate propensity scores&lt;/h1&gt;
&lt;p&gt;Below I create a dataset with only the covariates and use the &lt;code&gt;paste()&lt;/code&gt; function to create propensity score model equation. For the sake of this example, I only focus on including main effects of the covariates in the propensity score model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;covs &amp;lt;- imp_dat %&amp;gt;%
  select(sex:climate, math_score_T1)

equation_ps &amp;lt;- paste(&amp;quot;drop_status ~ &amp;quot;, paste(names(covs), collapse = &amp;quot; + &amp;quot;))
equation_ps&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;drop_status ~  sex + race + language + repeated_grade + IEP + locale + region + SES + math_identity + math_utility + math_efficacy + math_interest + engagement + belonging + expectations + climate + math_score_T1&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here, I create a function &lt;code&gt;estimate_ps()&lt;/code&gt; that takes in an equation and a dataset, then runs logistic regression using &lt;code&gt;glm()&lt;/code&gt;. Then the function adds logit of propensity scores and propensity scores as columns in the dataset.&lt;/p&gt;
&lt;p&gt;I then group the &lt;code&gt;imp_dat&lt;/code&gt; by the imputation number and then run the &lt;code&gt;estimate_ps()&lt;/code&gt; function on each of the imputed dataset using the &lt;code&gt;do()&lt;/code&gt; function from &lt;code&gt;dplyr&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;estimate_ps &amp;lt;- function(equation, dat){
  
  ps_model &amp;lt;- glm(as.formula(equation), family = binomial, data = dat)
  
  dat &amp;lt;- dat %&amp;gt;%
    mutate(ps_logit = predict(ps_model, type = &amp;quot;link&amp;quot;),
           ps = predict(ps_model, type = &amp;quot;response&amp;quot;))
  
  return(dat)
  
}


imp_dat_ps &amp;lt;- imp_dat %&amp;gt;%
  group_by(.imp) %&amp;gt;%
  do(estimate_ps(equation_ps, .)) %&amp;gt;%
  ungroup()&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;estimating-weights-using-the-across-and-within-methods&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Estimating weights using the across and within methods&lt;/h1&gt;
&lt;p&gt;Below I estimate ATT weights using the across and within methods. For the Across method, I use the average of the propensity scores across the imputed datasets to calculate weights. For the within method, I use the the propensity scores estimated within each imputed dataset to calculate weights.&lt;/p&gt;
&lt;p&gt;The code below groups the imputed data by &lt;code&gt;.id&lt;/code&gt; which is an identifier denoting each case. For each case, I summarize the mean of the propensity scores across the 10 imputed dataset. I then estimate the ATT weights using the averaged propensity scores for the Across method and the original propensity scores for the Within method.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;imp_dat_ps &amp;lt;- imp_dat_ps %&amp;gt;%
  group_by(.id) %&amp;gt;%
  mutate(ps_across = mean(ps)) %&amp;gt;%
  ungroup() %&amp;gt;%
  mutate(att_wt_across = drop_status + (1 - drop_status) * ps_across/(1 - ps_across),
         att_wt_within = drop_status + (1 - drop_status) * ps/(1 - ps))


imp_dat_ps %&amp;gt;%
  select(.imp, ps, ps_across, att_wt_across, att_wt_within)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 214,020 x 5
##     .imp       ps ps_across att_wt_across att_wt_within
##    &amp;lt;int&amp;gt;    &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;         &amp;lt;dbl&amp;gt;         &amp;lt;dbl&amp;gt;
##  1     1 0.00339   0.00330       0.00331       0.00340 
##  2     1 0.00989   0.0102        0.0103        0.00999 
##  3     1 0.00135   0.00131       0.00132       0.00135 
##  4     1 0.0119    0.0114        0.0115        0.0120  
##  5     1 0.00222   0.00246       0.00246       0.00222 
##  6     1 0.00289   0.00294       0.00295       0.00290 
##  7     1 0.0113    0.0101        0.0102        0.0114  
##  8     1 0.00347   0.00375       0.00377       0.00348 
##  9     1 0.00327   0.00276       0.00276       0.00328 
## 10     1 0.000667  0.000633      0.000633      0.000667
## # … with 214,010 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;estimate-att-for-across-and-within-methods&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Estimate ATT for across and within methods&lt;/h1&gt;
&lt;p&gt;Below I create a function to estimate the ATT. The function takes in an equation, a dataset, and weights as arguments. Then it runs a model using &lt;code&gt;lm_robust()&lt;/code&gt; from the &lt;code&gt;estimatr&lt;/code&gt; package. The standard errors of the regression coefficients are estimated using &lt;code&gt;HC2&lt;/code&gt; type sandwich errors. I then clean up the results using &lt;code&gt;tidy()&lt;/code&gt; from &lt;code&gt;broom&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;estimate_ATT &amp;lt;- function(equation, dat, wts){
  
  wts &amp;lt;- dat %&amp;gt;% pull({{wts}})
  
  model &amp;lt;- lm_robust(as.formula(equation), data = dat, weights = wts)
  
  res &amp;lt;- model %&amp;gt;%
    tidy() %&amp;gt;%
    filter(term == &amp;quot;drop_status&amp;quot;) %&amp;gt;%
    select(term, estimate, se = std.error, dci_low = conf.low, ci_high = conf.high, df = df)
  
  return(res)
  
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I set up an equation regressing the outcome variable, &lt;code&gt;math_score_T2&lt;/code&gt; on drop status and on the main effects of all the covariates. I then run the &lt;code&gt;estimate_ATT()&lt;/code&gt; function on each imputed data using &lt;code&gt;group_by()&lt;/code&gt; and &lt;code&gt;do()&lt;/code&gt;. The weights are different for the Across and Within methods.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;equation_ancova &amp;lt;- paste(&amp;quot;math_score_T2 ~ drop_status + &amp;quot;, paste(names(covs), collapse = &amp;quot; + &amp;quot;))
equation_ancova&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;math_score_T2 ~ drop_status +  sex + race + language + repeated_grade + IEP + locale + region + SES + math_identity + math_utility + math_efficacy + math_interest + engagement + belonging + expectations + climate + math_score_T1&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;across_res &amp;lt;- imp_dat_ps %&amp;gt;%
  group_by(.imp) %&amp;gt;%
  do(estimate_ATT(equation = equation_ancova, dat = ., wts = att_wt_across)) %&amp;gt;%
  ungroup()

within_res &amp;lt;- imp_dat_ps %&amp;gt;%
  group_by(.imp) %&amp;gt;%
  do(estimate_ATT(equation = equation_ancova, dat = ., wts = att_wt_within)) %&amp;gt;%
  ungroup()&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;pool-the-results&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Pool the results&lt;/h1&gt;
&lt;p&gt;Here, I create a function called &lt;code&gt;calc_pooled()&lt;/code&gt; using formula by (1999) to pool the results across the imputations. The &lt;code&gt;mice&lt;/code&gt; package has the &lt;code&gt;pool()&lt;/code&gt; function to do the same thing but we would need to convert the imputed data back to &lt;code&gt;mids&lt;/code&gt; object type and I just wanted to skip all that :D&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;calc_pooled &amp;lt;- function(dat, est, se, df){
  
  dat &amp;lt;- dat %&amp;gt;%
    mutate(est = dat %&amp;gt;% pull({{est}}),
           se = dat %&amp;gt;%pull({{se}}),
           df = dat %&amp;gt;% pull({{df}}))
  
  pooled &amp;lt;- dat %&amp;gt;%
    summarize(m = n(),
              B = var(est),  # between imputation var
              beta_bar = mean(est), # mean of estimated reg coeffs
              V_bar = mean(se^2), # mean of var - hc corrected   within imp var
              eta_bar = mean(df)) %&amp;gt;%   # mean of df
    mutate(
      
      V_total = V_bar + B * (m + 1) / m,  #between and within var est
      gamma = ((m + 1) / m) * B / V_total,  
      df_m = (m - 1) / gamma^2,
      df_obs = eta_bar * (eta_bar + 1) * (1 - gamma) / (eta_bar + 3),
      df = 1 / (1 / df_m + 1 / df_obs),
      
      # output
      se = sqrt(V_total),
      ci_lower = beta_bar - se * qt(0.975, df = df),
      ci_upper = beta_bar + se * qt(0.975, df = df)) %&amp;gt;%
    
    select(est = beta_bar, se, df, ci_lower, ci_upper) 
  
  return(pooled)
  
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;pooling&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Pooling&lt;/h1&gt;
&lt;p&gt;Below I use the &lt;code&gt;calc_pooled()&lt;/code&gt; function to pool the results for each of the methods.&lt;/p&gt;
&lt;div id=&#34;across&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Across&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;across_pooled &amp;lt;- calc_pooled(dat = across_res, est = estimate, se = se, df = df)
across_pooled %&amp;gt;%
  kable()&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;right&#34;&gt;est&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;se&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;df&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;ci_lower&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;ci_upper&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;-0.3353566&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0379174&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;93.77208&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.4106448&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.2600683&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;within&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Within&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;within_pooled &amp;lt;- calc_pooled(dat = within_res, est = estimate, se = se, df = df)
within_pooled %&amp;gt;%
  kable()&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;right&#34;&gt;est&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;se&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;df&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;ci_lower&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;ci_upper&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;-0.3557541&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0409797&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;50.00218&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.4380642&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.273444&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;which-to-use&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Which to use&lt;/h1&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Unemployment Claims COVID-19</title>
      <link>/post/unemployment_claims/</link>
      <pubDate>Sun, 21 Jun 2020 00:00:00 +0000</pubDate>
      <guid>/post/unemployment_claims/</guid>
      <description>


&lt;p&gt;In this post I am visualizing and analyzing the unprecedented increase in the number of unemployment claims filed in the US after the lockdown due to COVID 19 pandemic. I am retrieving the data from the &lt;code&gt;tidyquant&lt;/code&gt; package (Dancho &amp;amp; Vaughan, 2020).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(CausalImpact)
library(tidyverse)
library(scales)
library(tidyquant)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;icsa-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;ICSA Data&lt;/h2&gt;
&lt;p&gt;Initial unemployment claims from the first date available, 1967:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;icsa_dat &amp;lt;- &amp;quot;ICSA&amp;quot; %&amp;gt;%
  tq_get(get = &amp;quot;economic.data&amp;quot;,  
         from = &amp;quot;1967-01-07&amp;quot;) %&amp;gt;%
  rename(claims = price)


glimpse(icsa_dat)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Rows: 2,789
## Columns: 3
## $ symbol &amp;lt;chr&amp;gt; &amp;quot;ICSA&amp;quot;, &amp;quot;ICSA&amp;quot;, &amp;quot;ICSA&amp;quot;, &amp;quot;ICSA&amp;quot;, &amp;quot;ICSA&amp;quot;, &amp;quot;ICSA&amp;quot;, &amp;quot;ICSA&amp;quot;, &amp;quot;ICSA&amp;quot;…
## $ date   &amp;lt;date&amp;gt; 1967-01-07, 1967-01-14, 1967-01-21, 1967-01-28, 1967-02-04, 1…
## $ claims &amp;lt;int&amp;gt; 208000, 207000, 217000, 204000, 216000, 229000, 229000, 242000…&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;icsa_dat %&amp;gt;%
  ggplot(aes(x = date, y = claims)) + 
  geom_line(color = &amp;quot;blue&amp;quot;) + 
  scale_y_continuous(labels = comma) +
  labs(x = &amp;quot;Date&amp;quot;, y = &amp;quot;Claims&amp;quot;, subtitle = &amp;quot;As of June 21, 2020&amp;quot;) + 
  ggtitle(&amp;quot;Unemployment Claims: 1967 to 2020&amp;quot;) +
  theme_bw()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/unemployment_claims_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;comparison-to-2008-recession&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Comparison to 2008 Recession&lt;/h2&gt;
&lt;p&gt;In the graph below, I only selected 2008 to 2020. We can compare the unemployment claims during the 2008 recession to the number of claims filed during the COVID-19 lockdown. What is happening now is preposterous.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;icsa_dat %&amp;gt;%
  mutate(year = year(date)) %&amp;gt;%
  filter(year &amp;gt; 2007) %&amp;gt;%
  ggplot(aes(x = date, y = claims)) + 
  geom_line(color = &amp;quot;blue&amp;quot;) + 
  scale_y_continuous(labels = comma) +
  labs(x = &amp;quot;Date&amp;quot;, y = &amp;quot;Claims&amp;quot;, subtitle = &amp;quot;As of June 21, 2020&amp;quot;) + 
  ggtitle(&amp;quot;Unemployment Claims: 2008 to 2020&amp;quot;) +
  theme_bw()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/unemployment_claims_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;causal-inference&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Causal Inference&lt;/h2&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;
&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;
Sometimes &lt;a href=&#34;https://twitter.com/hashtag/causalinference?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#causalinference&lt;/a&gt; is simple.&lt;br&gt;&lt;br&gt;“What&#39;s the immediate causal effect of the &lt;a href=&#34;https://twitter.com/hashtag/COVID19?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#COVID19&lt;/a&gt; lockdowns on unemployment?”&lt;br&gt;&lt;br&gt;The answer is “Unprecedented”. &lt;br&gt;&lt;br&gt;We know we&#39;re in deep trouble when a time series is all we need. &lt;a href=&#34;https://t.co/cXK0wLw3no&#34;&gt;https://t.co/cXK0wLw3no&lt;/a&gt; &lt;a href=&#34;https://t.co/kS4PvVwihM&#34;&gt;pic.twitter.com/kS4PvVwihM&lt;/a&gt;
&lt;/p&gt;
— Miguel Hernán (&lt;span class=&#34;citation&#34;&gt;@_MiguelHernan&lt;/span&gt;) &lt;a href=&#34;https://twitter.com/_MiguelHernan/status/1244215937978576898?ref_src=twsrc%5Etfw&#34;&gt;March 29, 2020&lt;/a&gt;
&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;p&gt;Below, I use the &lt;code&gt;CausalImpact&lt;/code&gt; package to run a Bayesian structural time-series analysis (Brodersen et al., 2015). For more information on the package, please see this &lt;a href=&#34;https://cran.r-project.org/web/packages/CausalImpact/vignettes/CausalImpact.html&#34;&gt;vignette&lt;/a&gt;. Typically, it would be good to add covariates in the analysis but the data does not have any and given the rate of increase, I highly doubt that the inclusion of covariates would matter much. It would be interesting to compare the number of claims filed in US versus the number of claims filed in country with better social and economic security systems in place (perhaps the Netherlands). The impact of COVID-19 lockdowns on the number of unemployment claims is probably exacerbated by the lack of social and economic security in the US. In addition, due to employer based healthcare system in the US, millions of people have lost or are going to lose health insurance. Now more than every we need Medicare for All, $2000 a month stimulus, Green New Deal. The impact of climate change will be worse.&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;
&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;
Projected⬆️in unemployment:&lt;br&gt;&lt;br&gt;🇩🇪: 3.2%➡️5.9%&lt;br&gt;🇬🇧: 3.9%➡️7%&lt;br&gt;🇫🇷: 8.5%➡️12%&lt;br&gt;🇺🇸: 3.5%➡️32.1%&lt;br&gt;&lt;br&gt;Projected⬆️ in # of uninsured:&lt;br&gt;🇩🇪: 0&lt;br&gt;🇬🇧: 0&lt;br&gt;🇫🇷: 0&lt;br&gt;🇺🇸: At least 12 million&lt;br&gt;&lt;br&gt;Solution: Guarantee healthcare and paychecks like other wealthy countries do. &lt;a href=&#34;https://t.co/44ijS2evzL&#34;&gt;https://t.co/44ijS2evzL&lt;/a&gt;
&lt;/p&gt;
— Warren Gunnels (&lt;span class=&#34;citation&#34;&gt;@GunnelsWarren&lt;/span&gt;) &lt;a href=&#34;https://twitter.com/GunnelsWarren/status/1252607696303513605?ref_src=twsrc%5Etfw&#34;&gt;April 21, 2020&lt;/a&gt;
&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dates &amp;lt;- icsa_dat %&amp;gt;% 
  pull(date)

# create pre and post
pre_period &amp;lt;- c(dates[1], dates[2776])
post_period &amp;lt;- c(dates[2777], dates[length(dates)])

# make into dat
dat &amp;lt;- icsa_dat %&amp;gt;%
  select(date, y = claims)

# causal impact
impact &amp;lt;- CausalImpact(dat, pre_period, post_period)

sum_impact &amp;lt;- impact$summary %&amp;gt;%
  mutate(type = rownames(.)) %&amp;gt;%
  pivot_longer(cols = -type, 
               names_to = &amp;quot;stats&amp;quot;,
               values_to = &amp;quot;vals&amp;quot;) 

avg_impact &amp;lt;- sum_impact %&amp;gt;%
  mutate(vals = round(vals/1000000, 2))

rel_impact &amp;lt;- sum_impact %&amp;gt;%
  filter(str_detect(stats, &amp;quot;Rel&amp;quot;)) %&amp;gt;%
  mutate(vals = round(vals * 100))

# summary(impact, &amp;quot;report&amp;quot;)  &lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;analysis-report-causalimpact&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Analysis report: CausalImpact&lt;/h2&gt;
&lt;p&gt;Below is the report generated by &lt;code&gt;CausalImpact&lt;/code&gt; with some edits by me.&lt;/p&gt;
&lt;p&gt;Summing up the individual data points during the post-lockdown period, the total number of unemployment claims filed equaled 45.74M. By contrast, had the intervention not taken place, we would have expected a sum of 3.28M. The 95% interval of this prediction is [2.7M, 3.84M].&lt;/p&gt;
&lt;p&gt;The probability of obtaining this effect by chance is very small (Bayesian one-sided tail-area probability p = 0.001). This means the causal effect can be considered statistically significant.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;p&gt;Brodersen et al., 2015, Annals of Applied Statistics. Inferring causal impact using Bayesian
structural time-series models. &lt;a href=&#34;http://research.google.com/pubs/pub41854.html&#34; class=&#34;uri&#34;&gt;http://research.google.com/pubs/pub41854.html&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Dancho, M. and Vaughan, D. (2020). tidyquant: Tidy Quantitative Financial Analysis. R
package version 1.0.0. &lt;a href=&#34;https://CRAN.R-project.org/package=tidyquant&#34; class=&#34;uri&#34;&gt;https://CRAN.R-project.org/package=tidyquant&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Wickham, H. and Seidel, D. (2019). scales: Scale Functions for Visualization. R package
version 1.1.0. &lt;a href=&#34;https://CRAN.R-project.org/package=scales&#34; class=&#34;uri&#34;&gt;https://CRAN.R-project.org/package=scales&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Wickham et al., (2019). Welcome to the tidyverse. Journal of Open Source Software, 4(43),
1686, &lt;a href=&#34;https://doi.org/10.21105/joss.01686&#34; class=&#34;uri&#34;&gt;https://doi.org/10.21105/joss.01686&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
