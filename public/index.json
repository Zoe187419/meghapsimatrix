[{"authors":["admin"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"","tags":null,"title":"Megha Joshi","type":"authors"},{"authors":null,"categories":null,"content":" Flexibility This feature can be used for publishing content such as:\n Online courses Project or software documentation Tutorials  The courses folder may be renamed. For example, we can rename it to docs for software/project documentation or tutorials for creating an online course.\nDelete tutorials To remove these pages, delete the courses folder and see below to delete the associated menu link.\nUpdate site menu After renaming or deleting the courses folder, you may wish to update any [[main]] menu links to it by editing your menu configuration at config/_default/menus.toml.\nFor example, if you delete this folder, you can remove the following from your menu configuration:\n[[main]] name = \u0026quot;Courses\u0026quot; url = \u0026quot;courses/\u0026quot; weight = 50  Or, if you are creating a software documentation site, you can rename the courses folder to docs and update the associated Courses menu configuration to:\n[[main]] name = \u0026quot;Docs\u0026quot; url = \u0026quot;docs/\u0026quot; weight = 50  Update the docs menu If you use the docs layout, note that the name of the menu in the front matter should be in the form [menu.X] where X is the folder name. Hence, if you rename the courses/example/ folder, you should also rename the menu definitions in the front matter of files within courses/example/ from [menu.example] to [menu.\u0026lt;NewFolderName\u0026gt;].\n","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536451200,"objectID":"59c3ce8e202293146a8a934d37a4070b","permalink":"/courses/example/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/courses/example/","section":"courses","summary":"Learn how to use Academic's docs layout for publishing online courses, software documentation, and tutorials.","tags":null,"title":"Overview","type":"docs"},{"authors":null,"categories":null,"content":" In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 2 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"74533bae41439377bd30f645c4677a27","permalink":"/courses/example/example1/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example1/","section":"courses","summary":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim.","tags":null,"title":"Example Page 1","type":"docs"},{"authors":null,"categories":null,"content":" Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 4 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"1c2b5a11257c768c90d5050637d77d6a","permalink":"/courses/example/example2/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example2/","section":"courses","summary":"Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus.","tags":null,"title":"Example Page 2","type":"docs"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature.   Slides can be added in a few ways:\n Create slides using Academic\u0026rsquo;s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.  Further talk details can easily be added to this page using Markdown and $\\rm \\LaTeX$ math code.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"96344c08df50a1b693cc40432115cbe3","permalink":"/talk/example/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example/","section":"talk","summary":"An example talk using Academic's Markdown slides feature.","tags":[],"title":"Example Talk","type":"talk"},{"authors":null,"categories":["R"],"content":"    In this post, I am analyzing data on COVID-19 cases worldwide. The data that I am analyzing only accounts for reported cases. Due to the lack of availablity and accessibility of testing in countries like the US, the numbers may be an underestimate of the actual number of cases, number of deaths, and number of recoveries. The data does not account for any other errors of measurement.\nLibraries library(tidyverse) library(maps) library(ggthemes) library(jsonlite) library(leaflet) library(widgetframe) library(lubridate) library(scales) library(ggrepel)  Read in the Data I downloaded the data from here.\nconfirmed \u0026lt;- read_csv(\u0026quot;https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv\u0026quot;) %\u0026gt;% pivot_longer(cols = -c(1:4), names_to = \u0026quot;date\u0026quot;, values_to = \u0026quot;confirmed\u0026quot;) deaths \u0026lt;- read_csv(\u0026quot;https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_deaths_global.csv\u0026quot;) %\u0026gt;% pivot_longer(cols = -c(1:4), names_to = \u0026quot;date\u0026quot;, values_to = \u0026quot;deaths\u0026quot;) recovered \u0026lt;- read_csv(\u0026quot;https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_recovered_global.csv\u0026quot;) %\u0026gt;% pivot_longer(cols = -c(1:4), names_to = \u0026quot;date\u0026quot;, values_to = \u0026quot;recoveries\u0026quot;) # Join and Clean dat \u0026lt;- left_join(confirmed, deaths) %\u0026gt;% left_join(recovered) %\u0026gt;% mutate(date = mdy(date), lab = paste0(\u0026quot;Confirmed: \u0026quot;, confirmed, \u0026quot;, \u0026quot;, \u0026quot;Deaths: \u0026quot;, deaths, \u0026quot;, \u0026quot;, \u0026quot;Recovered: \u0026quot;, recoveries)) %\u0026gt;% rename(country_region = `Country/Region`)  Coronavirus Confirmed Cases covid_leaf \u0026lt;- leaflet(data = dat, options = leafletOptions(minZoom = 1.5)) %\u0026gt;% addProviderTiles(providers$CartoDB.DarkMatter) %\u0026gt;% addCircleMarkers(~Long, ~Lat, radius = ~confirmed/10000, label = ~lab, fillOpacity = .2) frameWidget(covid_leaf)  {\"x\":{\"url\":\"/post/Coronavirus_files/figure-html//widgets/widget_unnamed-chunk-3.html\",\"options\":{\"xdomain\":\"*\",\"allowfullscreen\":false,\"lazyload\":false}},\"evals\":[],\"jsHooks\":[]}  Cases by Week Confirmed country_dat \u0026lt;- dat %\u0026gt;% filter(country_region %in% c(\u0026quot;China\u0026quot;, \u0026quot;Italy\u0026quot;, \u0026quot;Iran\u0026quot;, \u0026quot;Korea, South\u0026quot;, \u0026quot;Spain\u0026quot;, \u0026quot;US\u0026quot;)) %\u0026gt;% mutate(week = cut(date, \u0026quot;week\u0026quot;, start.on.monday = FALSE)) country_dat %\u0026gt;% ggplot(aes(x = confirmed/1000, y = week)) + geom_bar(stat = \u0026quot;identity\u0026quot;, position = \u0026quot;dodge\u0026quot;, fill = \u0026quot;blue4\u0026quot;) + facet_wrap(~ country_region) + labs(y = \u0026quot;\u0026quot;, x = \u0026quot;Confirmed Cases (in thousands)\u0026quot;) + scale_x_continuous(labels = comma) + theme_bw() + theme(legend.position = \u0026quot;none\u0026quot;)  Deaths country_dat %\u0026gt;% ggplot(aes(x = deaths/1000, y = week)) + geom_bar(stat = \u0026quot;identity\u0026quot;, position = \u0026quot;dodge\u0026quot;, fill = \u0026quot;red4\u0026quot;) + facet_wrap(~ country_region) + labs(y = \u0026quot;\u0026quot;, x = \u0026quot;Deaths (in thousands)\u0026quot;) + scale_x_continuous(labels = comma) + theme_bw() + theme(legend.position = \u0026quot;none\u0026quot;)  Recoveries country_dat %\u0026gt;% ggplot(aes(x = recoveries/1000, y = week)) + geom_bar(stat = \u0026quot;identity\u0026quot;, position = \u0026quot;dodge\u0026quot;, fill = \u0026quot;green4\u0026quot;) + facet_wrap(~ country_region) + labs(y = \u0026quot;\u0026quot;, x = \u0026quot;Recovered (in thousands)\u0026quot;) + scale_x_continuous(labels = comma) + theme_bw() + theme(legend.position = \u0026quot;none\u0026quot;)   Confirmed Cases Trend sum_dat \u0026lt;- country_dat %\u0026gt;% group_by(date, country_region) %\u0026gt;% summarize(confirmed = sum(confirmed)) %\u0026gt;% ungroup() %\u0026gt;% mutate(country_region = if_else(country_region == \u0026quot;Korea, South\u0026quot;, \u0026quot;S. Korea\u0026quot;, country_region)) sum_dat %\u0026gt;% ggplot(aes(x = date, y = confirmed, color = country_region)) + geom_line(size = .8) + geom_text(data = sum_dat %\u0026gt;% filter(date == last(date)), aes(label = country_region, x = date + .1, y = confirmed, color = country_region), hjust = -.01) + scale_y_continuous(labels = comma, breaks = seq(0, 700000, 50000)) + xlim(ymd(\u0026quot;2020-01-22\u0026quot;), ymd(\u0026quot;2020-04-20\u0026quot;)) + scale_color_brewer(type = \u0026quot;qual\u0026quot;, palette = 2) + labs(x = \u0026quot;\u0026quot;, y = \u0026quot;Confirmed Cases\u0026quot;, color = \u0026quot;Country\u0026quot;) + ggtitle(\u0026quot;Confirmed Cases as of April 15, 2020\u0026quot;) + theme_bw() + theme(legend.position = \u0026quot;none\u0026quot;)  ","date":1586908800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586908800,"objectID":"b9c0dffc608e70ade7898d5a98a871b6","permalink":"/post/coronavirus/","publishdate":"2020-04-15T00:00:00Z","relpermalink":"/post/coronavirus/","section":"post","summary":"In this post, I am analyzing data on COVID-19 cases worldwide. The data that I am analyzing only accounts for reported cases. Due to the lack of availablity and accessibility of testing in countries like the US, the numbers may be an underestimate of the actual number of cases, number of deaths, and number of recoveries. The data does not account for any other errors of measurement.","tags":["coronavirus","COVID-19","ggplot","leaflet"],"title":"COVID-19","type":"post"},{"authors":null,"categories":["R"],"content":" Theories behind propensity score analysis assume that the covariates are fully observed (Rosenbaum \u0026amp; Rubin, 1983, 1984). However, in practice, observational analyses require large administrative databases or surveys, which inevitably will have missingness in the covariates. The response patterns of people with missing covariates may be different than those of people with observed data (Mohan, Pearl, \u0026amp; Tian, 2013). Therefore, ways to handle missing covariate data need to be examined. The basic estimation of propensity scores using logistic regression will delete cases with missing data, which can be problematic as it can cause bias in the treatment effect estimates (Baraldi \u0026amp; Enders, 2010). Below, I review the three missing data mechanisms introduced by Rubin (1976) that provide the foundation for how missing data is handled in applied research.\nMissing Data Mechanisms Mohan et al. (2013) and Thoemmes \u0026amp; Mohan (2015) used conditional independence statements to define each of the missing data mechanisms. I describe the mechanisms using the conditional independence notations as they help clarify relationships between missing data, missingness patterns, and observed study variables. Let \\(X\\) denote an \\(n \\times k\\) matrix of study variables, that can be partitioned into \\(X_{obs}\\), representing the observed part of the variables, and \\(X_{mis}\\), representing the unobserved part. Let \\(R\\) denote an \\(n \\times k\\) matrix of binary missingness indicators, equal to 1 if a variable is missing and 0 if the variable is observed. Let \\(X_j\\) denote the \\(j^{th}\\) variable in \\(X\\).\nMissing Completely at Random The Missing Completely at Random (MCAR) condition requires the probability of missingness on a variable, \\(X_j\\), to be unrelated to other observed variables and to the hypothetically complete values of \\(X_j\\) (Baraldi \u0026amp; Enders, 2010). Under MCAR, \\[\\begin{equation} \\label{eq:mcar1} P(R|X) = P(R|X_{obs}, X_{mis}) = P(R) \\end{equation}\\] \\[\\begin{equation} \\label{eq:mcar2} R \\perp\\!\\!\\!\\perp (X_{obs}, X_{mis}) \\end{equation}\\] The unconditional distribution of missingness, \\(P(R)\\), equals the distribution of \\(R\\) conditioned on the observed and unobserved parts of \\(X\\) (Mohan et al., 2013; Thoemmes \u0026amp; Mohan, 2015). Missingness is independent of both the observed and unobserved parts of \\(X\\) (Mohan et al., 2013; Thoemmes \u0026amp; Mohan, 2015). If the MCAR assumption holds, the sample without missing values is a random subsample of the original full-data sample (Baraldi \u0026amp; Enders, 2010).\nThoemmes \u0026amp; Mohan (2015) note that, under MCAR, missingness occurs due to accidental reasons. Examples of MCAR include participants neglecting to answer items on a survey randomly and random data reading errors that generate missing values (Baraldi \u0026amp; Enders, 2010; Thoemmes \u0026amp; Mohan, 2015). The MCAR assumption is very restrictive and also unlikely to be met in applied research (Baraldi \u0026amp; Enders, 2010; Thoemmes \u0026amp; Mohan, 2015). Although tests can verify that the data are not MCAR, tests cannot conclusively prove that the data are MCAR because the complete dataset is not observed (Baraldi \u0026amp; Enders, 2010; Thoemmes \u0026amp; Mohan, 2015). Complete case analysis, which entails deleting any case with missing data, will provide unbiased estimates if the data are MCAR (Thoemmes \u0026amp; Mohan, 2015). However, if large proportion of the data are missing, deleting cases can result in loss of power.\n Missing at Random The Missing at Random (MAR) condition, which is less restrictive than the MCAR assumption, requires that the probability of missingness on \\(X_j\\) is not related to the hypothetically complete values of \\(X_j\\) after controlling for the other observed variables (Baraldi \u0026amp; Enders, 2010). Under MAR, \\[\\begin{equation} \\label{eq:mar1} P(R|X) = P(R|X_{obs}, X_{mis}) = P(R|X_{obs}) \\end{equation}\\] \\[\\begin{equation} \\label{eq:mar2} R \\perp\\!\\!\\!\\perp X_{mis}|X_{obs} \\end{equation}\\] The distribution of missingness given observed and unobserved parts of \\(X\\) equals the distribution of \\(R\\) conditioned on the observed parts of \\(X\\) (Thoemmes \u0026amp; Mohan, 2015). Missingness is independent of the unobserved parts of \\(X\\) given the observed data (Mohan et al., 2013; Thoemmes \u0026amp; Mohan, 2015).\nAn example of MAR would be if students who have higher student residential mobility are less likely to report prior math achievement scores. These students are also likely to have lower achievement scores. After controlling for residential mobility which is observed in the study data, missingness in achievement is no longer related to the hypothetically complete values of achievement. The MAR assumption is untestable as the hypothetically complete values of the variables would not be observed. Full information maximum likelihood estimation and multiple imputation, which make use of the information available in \\(X_{obs}\\), are appropriate methods to use when the data are MAR or MCAR (Thoemmes \u0026amp; Mohan, 2015).\n Missing Not at Random The Missing Not at Random (MNAR) condition implies that the probability of missingness on a variable depends on the hypothetically complete variable even after controlling for other observed variables (Baraldi \u0026amp; Enders, 2010). Neither of the independences implied by MCAR or MAR apply for data that are MNAR (Thoemmes \u0026amp; Mohan, 2015): \\[\\begin{equation} \\label{eq:mnar1} P(R|X_{obs}, X_{mis}) \\neq P(R|X_{obs}) \\end{equation}\\]\nAn example of MNAR would be students of lower socioeconomic status (SES) not reporting their SES. If, even after controlling for other predictors of SES like race and parent’s education level, missingness in SES is still related to SES, then the data are MNAR. MNAR can also occur if missingness is related to an unobserved variable that is correlated with the hypothetically complete values of \\(X\\) (Thoemmes \u0026amp; Mohan, 2015). In the case of the achievement and residential mobility example for MAR, if residential mobility is not observed, a dependency is introduced between achievement and its missingness. Like the prior two mechanisms, MNAR is also unverifiable. Analysis methods that try to address MNAR include selection models and pattern mixture models. Selection models include a model to predict \\(R\\) combined with a model to predict \\(X\\) (Little, 2008). Pattern mixture models estimate parameters separately for each missing data pattern and then average the estimates across the patterns to derive the final estimate (Little, 2008). These methods rely on heavy assumptions like multivariate normality and may result in severely biased parameter estimates when these assumptions are not met (Baraldi \u0026amp; Enders, 2010).\n  Missing Potential Outcomes Here, I briefly connect the other underlying layer of missing data in causal inference, the missing potential outcomes, to the missing data mechanisms. In the context of randomized experiments, potential outcomes can be thought of as being MCAR. A simple randomized experiment ensures that the treatment groups are not systematically different in terms of any variables and that the potential outcomes are independent of treatment group. Therefore, the parts of the population that have observed \\(Y(1)\\) and \\(Y(0)\\), the treated and the untreated group respectively, can be considered as a simple random sample of the full population (Gerber \u0026amp; Green, 2012). Because MCAR can be assumed, taking the simple average of the outcome in each of the group, deleting the cases that do not have observed potential outcomes, and taking the difference of the averages lets us retrieve the causal effect estimate (Gerber \u0026amp; Green, 2012; Schafer \u0026amp; Kang, 2008). In context of observational studies, potential outcomes can be thought of as being MAR. The difference in the means of the potential outcomes can be retrieved after conditioning on the fully observed \\(X\\) (Schafer \u0026amp; Kang, 2008). This paper focuses on situations when \\(X\\) is not fully observed and how to properly handle missingness in \\(X\\).\n Missing Data Methods in Propensity Score Analysis Below I explain three major methods used in the applied propensity score analysis literature when \\(X\\) is not fully observed. I also explain three other methods to handle missing data that are not commonly used in applied literature but have been proposed theoretically. I also describe the assumptions about missing data and strong ignorability underlying each of the methods. Let \\(X_{obs}\\) indicate the observed parts of \\(X\\) and \\(X_{mis}\\) indicate the missing parts of \\(X\\). \\(D\\) indicates the fully observed treatment indicator and \\(Y\\) indicates a fully observed outcome variable.\nComplete Case Analysis This approach deletes cases with missing data in any of the variables used in the analysis (Baraldi \u0026amp; Enders, 2010; Hill, 2004). The traditional propensity score estimation method of using logistic regression implements complete case analysis by default. Therefore, this method is commonly used in applied research. The data that remains after deleting cases with missing data are assumed to be a simple random sample of the full data (Baraldi \u0026amp; Enders, 2010). Missingness is not related to any study variables nor to the hypothetically complete values of itself (Equations and ). According to Hill (2004), the assumption underlying complete case analysis is that the joint distributions of \\(X_{obs}\\) and \\(X_{mis}\\) are same across the two treatment conditions: \\[\\begin{equation} X_{obs}, X_{mis} \\perp\\!\\!\\!\\perp D \\end{equation}\\] Therefore, an unbiased causal effect estimate can be retrieved after deleting cases with missing data. Such an assumption is very stringent and unlikely to be met in the types of data required for propensity score analyses (Baraldi \u0026amp; Enders, 2010; Hill, 2004). As mentioned above, deleting cases can also result in loss of power. Additionally, whether \\(X_{mis}\\) is balanced between the treatment groups cannot be confirmed.\n Multiple Imputation Multiple imputation (MI) generates multiple sets of data with the missing values drawn from an imputation model (Mitra \u0026amp; Reiter, 2016; Rubin, 1987). MI will create \\(m \u0026gt; 1\\) imputed datasets that contain different imputed values (Murray, 2018; van Buuren, 2018). Analyses can be performed on each of the datasets and results from each dataset can be aggregated across to derive a final estimate, standard error, degrees of freedom, and test result. Thus, MI involves two stages: (1) imputation and creation of the \\(m\\) imputed datasets, and (2) analysis and pooling of estimates across the datasets (Murray, 2018; van Buuren, 2018).\nThere are two approaches for imputing multivariate missing data: (1) joint modeling, JM, and (2) fully conditional specification, FCS, also called multivariate imputation by chained equations, MICE (Murray, 2018; van Buuren, 2018; van Buuren \u0026amp; Groothuis-Oudshoorn, 2011). JM entails jointly modeling variables with missingness by drawing from a multivariate distribution (Murray, 2018; van Buuren, 2018; van Buuren \u0026amp; Groothuis-Oudshoorn, 2011). FCS entails univariate conditional imputation models of variables with missing data that iteratively condition on all other variables using Monte Carlo Markov chain methods (van Buuren, 2018; van Buuren \u0026amp; Groothuis-Oudshoorn, 2011). JM imputes all variables simultaneously whereas FCS imputes one variable at a time (van Buuren, 2018). Because JM requires specification of a joint distribution for all the variables, it may not be as flexible as FCS when dealing with a large number of covariates with missing data (Akande, Li, \u0026amp; Reiter, 2017). However, FCS is computationally more intensive than JM (van Buuren, 2018). FCS also has been shown to outperform JM for categorical variables and is more robust under mis-specification of imputation model (van Buuren, 2018). Therefore, van Buuren (2018) recommended to use FCS over JM.\nIf the missingness mechanism is MAR or MCAR and if assumptions underlying the imputation model are correct, MI will yield unbiased results, as it uses the information available in \\(X_{obs}\\) to impute missing values (Murray, 2018). In the causal inference context, Hill (2004) argued that MI relies on the assumption of latent ignorability, a concept introduced by Frangakis \u0026amp; Rubin (1999). The assumption requires that the treatment assignment mechanism is ignorable given complete covariate data including the values that are latent or missing. These missing values are derived from MI. Below, let \\(e_{MI}(X)\\) denote propensity scores derived after multiple imputation: \\[\\begin{equation} X_{obs}, X_{mis} \\perp\\!\\!\\!\\perp D| e_{MI}(X) \\end{equation}\\] \\[\\begin{equation} Y(1), Y(0) \\perp\\!\\!\\!\\perp D | e_{MI}(X) \\end{equation}\\] Hill (2004) proposed two different ways to combine propensity scores estimated in each of the m datasets:\nMultiple Imputation Across (MI Across) This approach involves creating m imputed datasets and then estimating propensity scores within each of the datasets and then averaging each unit’s m propensity scores across the m datasets (Hill, 2004). Stratification, matching or IPW can be implemented using these averaged propensity scores (Hill, 2004). Outcome models that include covariates will need to use the weights or strata derived from the averaged propensity scores and the m sets of covariate values. The weighted regression estimates will then need to be pooled.\n Multiple Imputation Within (MI Within) This approach involves creating m imputed datasets and then estimating propensity scores within each of the datasets (Hill, 2004). Instead of averaging the propensity scores across the datasets, this method entails conditioning on the propensity scores within the datasets and running the outcome analyses within each dataset (Hill, 2004). The separate regression estimates have to be pooled.\n  Generalized Propensity Scores Rosenbaum \u0026amp; Rubin (1984) proposed the use of generalized propensity scores (GPS) as a way to address missing covariate data. The GPS represents the probability of treatment given observed covariates and missingness indicators (Rosenbaum \u0026amp; Rubin, 1984): \\[\\begin{equation} e^*(X) = P(D = 1|X_{obs}, R) \\end{equation}\\] Conditioning on \\(e^*(X)\\) will balance the treatment groups in terms of the observed covariates and missingness patterns (Rosenbaum \u0026amp; Rubin, 1984). The observed part of \\(X\\) and the missingness pattern indicators, \\(R\\), will be independent of treatment assignment given the GPS (Rosenbaum \u0026amp; Rubin, 1984): \\[\\begin{equation} X_{obs}, R \\perp\\!\\!\\!\\perp D| e^*(X) \\end{equation}\\] However, conditioning on GPS will not balance the groups in terms of the unobserved values of \\(X\\) (Rosenbaum \u0026amp; Rubin, 1984): \\[\\begin{equation} X_{mis} \\not\\!\\perp\\!\\!\\!\\perp D| e^*(X) \\end{equation}\\] Although this technique of treating missing data is not generally recommended for other types of missing data analyses, it has been recommended for use in propensity score analysis literature (Rosenbaum \u0026amp; Rubin, 1984; Stuart, 2010). In the context of propensity score analysis, this approach does not assume latent ignorability of treatment assignment because legitimate values for missing data are never derived. The assumption underlying this method is that balancing the treatment and control groups on \\(X_{obs}\\) and \\(R\\) is a sufficient condition to satisfy ignorability. With the GPS, the treatment and control groups are possibly not going to be balanced in terms of \\(X_{mis}\\).\nFor large studies with few missing data patterns, Rosenbaum \u0026amp; Rubin (1984) suggested estimating separate logit models for each missingness pattern. In practice, it is common to encounter many patterns of missing data. For these scenarios, Rosenbaum \u0026amp; Rubin (1984) suggested creating an additional category indicating missingness for categorical variables. For continuous variables, Stuart (2010) recommended imputing missing data with a single arbitrary value, such as the overall mean of the covariate, and then creating a missingness indicator variable. In general missing data analysis context, van Buuren (2018) noted that this method of combining arbitrary (mean) imputation along with missingness indicators can underestimate the standard error of the estimate of interest.\nThe CART algorithms treat missing data natively as they split missingness as a category itself. In this manner, this approach is similar to the GPS which uses missingness pattern indicators when estimating propensity scores. The missingness categories are used to estimate propensity scores and conditioning on the propensity scores should balance the treatment and control condition in terms of the patterns. However, splitting does not actually impute the missing data so it is plausible to assume that like GPS, scores derived using the splitting method will not balance the groups in terms of the latent missing data. In addition, unlike MI, there are no imputed complete datasets saved to analyze for the outcome model. Therefore, splitting would need to be combined with some other technique for outcome modeling.\n Other Methods The following methods have been discussed theoretically in literature examining missing data methods in propensity score analysis. However, these methods are not commonly used in applied literature.\nComplete Variables This method removes any variable with missing data (Hill, 2004). By removing variables with missing data, the approach assumes that the distribution of those variables (both the observed and missing parts) are the same across the two treatment groups (Hill, 2004). If this assumption does not hold, then this method can result in bias in treatment effect estimates due to removal of important confounding variables (Hill, 2004).\n D’Agostino and Rubin Expectation Maximization Another approach is a method introduced by D’Agostino \u0026amp; Rubin (2000), which estimates propensity scores using an Expectation Conditional Maximization (ECM) algorithm (Hill, 2004). This method, DR, works similar to GPS as it models \\(X_{obs}\\), \\(R\\), and the treatment indicator variable. However, instead of imputing \\(X_{mis}\\), the DR method uses ECM to estimate propensity scores in presence of missing data (Hill, 2004). The assumption underlying DR is that within each missingness pattern defined by \\(R\\), \\(X_{mis}\\) is independent of \\(D\\) given the observed data, \\(X_{obs}\\) (Hill, 2004): \\[\\begin{equation} X_{mis} \\perp\\!\\!\\!\\perp D| X_{obs}, R \\end{equation}\\] Such independence is sufficient to satisfy the ignorability assumption in presence of missing covariate data. With this method, the assumption cannot be checked, however, as DR does not actually impute the missing values. This method is not readily available in commonly used software like R.\n Multiple Imputation Missingness Indicator Pattern Mixutre Qu \u0026amp; Lipkovich (2009) extended MI by introducing the missingness indicator pattern mixture (MIMP) approach, which is the same as MI but adds \\(R\\) in the propensity score estimation model. The rationale behind this approach is to use information given by missingness patterns to estimate treatment propensities. The method will assume latent ignorabilty. However, this approach should also balance the treatment group on \\(R\\) as \\(R\\) is used to estimate the propensity scores: \\[\\begin{equation} X_{obs}, X_{mis}, R \\perp\\!\\!\\!\\perp D| e_{MIMP}(X) \\end{equation}\\] Qu \u0026amp; Lipkovich (2009) argued that extending MI by adding R to the propensity score estimation accounts for non-ignorability or MNAR (Qu \u0026amp; Lipkovich, 2009; van Buuren, 2018). This method allows missingness itself to provide information on missingness: \\[\\begin{equation} P(X| X_{obs}, R = 1) \\neq P(X| X_{obs}, R = 0) \\end{equation}\\]\n   Performance of Missing Data Methods in Propensity Score Analysis Few methodological studies have examined the performance of missing data methods when using propensity score analysis for causal inference. Hill (2004), an unpublished working paper, compared complete case analysis, complete variables analysis, DR, and the two MI methods including and excluding the outcome when imputing data. The generated data included eight covariates, three categorical, drawn from multinominal distribution, and five continuous, drawn from multivariate normal distribution. Ignorable (MAR) missingness was simulated based on \\(X_{obs}\\). Missingness rates ranged from 13% to 37%. A binary treatment variable was simulated as a linear function of the covariates, with an average of 23% of each sample in the treatment condition. Potential outcomes were generated with non-parallel response surface throughout the covariate space (treatment effect heterogeneity was present) and the outcomes were predicted by four of the covariates. Hill (2004) used the matching method and calculated the difference in means of the outcomes between the treated and the untreated groups to estimate the ATT. The sample size per simulation iteration was 1000 and the number of imputations used for MI methods was 10.\nHill (2004) calculated percent reduction in bias defined as the reduction in the absolute difference in means of the covariates between the treatment groups before matching compared to after matching divided by the absolute initial difference in means. Results from Hill (2004) showed that complete case analysis increased covariate imbalance between the treatment groups. Complete variables approach resulted in even worse imbalance, especially for the variables with missing data. MI techniques performed better than DR with MI Across including the outcome showing the best percent reduction in bias for balance diagnostics. In terms of absolute bias for the treatment effect estimate, MI Across without the outcome performed best. In terms of mean squared error, MI Across with the outcome performed best. The MI techniques were recommended over DR, complete case, and complete variable analysis.\nMitra \u0026amp; Reiter (2016) conducted a similar study comparing the two different methods of multiple imputation. The outcome variable was not considered during imputation (to adhere to the spirit of propensity scores as being blind to the outcome). The generated data included two covariates one of which contained missing data generated following MAR mechanism. The sample size for each simulated data set contained approximately 100 treated units and 1000 untreated units. Treatment indicators were generated as a linear function of the complete variable only, of the variable with missing data, and of both of the variables. The number of simulation iterations conducted was 1000. One-to-one matching method was used to condition on the propensity scores and the ATT was estimated by taking the simple difference in the means of the outcome variables between the treated and the matched untreated group. MI Across showed greater bias reduction than MI Within, especially in conditions where the variable with missingness predicted treatment assignment.\nQu \u0026amp; Lipkovich (2009) compared the MIMP approach that they introduced to complete case analysis, the MI method, and a pattern mixture (MP) method that is similar to the GPS except the authors combined patterns that contained few cases. Twelve covariates were generated from a compound symmetric multivariate normal distribution. The treatment assignment indicator was generated as a linear function of the covariates. The outcome was generated as a linear function of the covariates and the treatment indicator variable. Ignorable (MCAR and MAR) and non-ignorable (MNAR) missingness were simulated based on linear combinations of the covariates. The number of simulation iterations conducted was 5000 and the number of imputations generated for MI was 5. The results showed that the complete case analysis removed bias under all three missingness conditions. I note that this result is counter-intuitive as theoretically, complete case analysis should introduce bias especially under MNAR. Under MAR, all the methods showed low bias. Under MNAR, MP and MIMP showed lower bias than MI.\nLeyrat et al. (2019) compared complete case analysis, GPS, the two MI methods, and an MI approach that averages the covariate values across the imputed datasets and then estimates the propensity scores. For MI, they experimented including the outcome and not including the outcome in the imputation model, similar to Hill (2004). The authors generated three covariates from a multivariate normal distribution, with one dichotomized. Treatment assignment was generated as a linear function of all three covariates. A binary outcome was generated as a linear function of the covariates and the treatment assignment indicator. The authors calculated log relative risk, log odds ratio and risk difference using IPW to condition on the propensity scores. MAR missingness was generated in the first and the third (binary) covariate as a function of the fully observed second covariate, treatment indicator and the outcome. The authors experimented with making missingness dependent and independent of the outcome. The authors did not include any covariates in the outcome model. For each condition, 5000 datasets with a sample size of 2000 (30% in the treatment group) each were generated. The results indicated that complete case analysis and GPS resulted in strong absolute bias in the treatment effect estimates. MI methods showed lower bias. Including the outcome in imputation model resulted in lower absolute bias even in cases where the outcome did not predict missingness. MI Across with the outcome included showed the lowest absolute bias (near zero). All of the MI methods showed adequate coverage rates whereas the complete case and GPS methods showed very large bias so their coverage rates were also inadequate. The authors recommended MI Across with the outcome included in the imputation model.\nCrowe, Lipkovich, \u0026amp; Wang (2010) conducted a simulation study comparing complete case analysis, treatment mean imputation (imputing missing values with the mean of the variable in the treatment group), and three types of MI Within. The three types entailed: (1) including only the covariates in the imputation model; (2) including the covariates and the treatment indicator in the imputation model; (3) including the covariates, treatment indicator and the outcome variable in the imputation model. The authors generated data for 2000 subjects with six continuous covariates from a compound symmetric multivariate normal distribution, with correlation between covariates equal to 0.3. The treatment assignment indicator was generated as a linear function of only one of the covariates and a binary outcome was generated as a linear function of that covariate and the treatment assignment indicator. MCAR and MAR missingness were generated that affected three of the covariates including the one that influenced both treatment assignment and outcome. The number of simulation iterations per condition was 3000. Results showed bias in treatment effect estimates even in conditions with no missing data and no confounding. This result is odd as theoretically there should be no bias in these conditions. Complete case analysis and MI that included all covariates, the treatment indicator and the outcome variable produced estimates that were not significantly different from estimates obtained under no missing data condition. Treatment mean imputation produced slightly larger bias under MCAR, and even larger bias under MAR. The other two MI methods (including covariates only, and including covariates and the treatment indicator) resulted in substantial bias.\n Leyrat et al. (2019) and Crowe et al. (2010) examined missing data methods to estimate treatment effects for a binary outcome variable. The effect estimation procedure for a binary outcome is different from that for a continuous outcome, which is the focus of the present study. Therefore, the results from these studies may not generalize to the context of my study. The results from Hill (2004), Mitra \u0026amp; Reiter (2016), and Qu \u0026amp; Lipkovich (2009) are more relevant.\nHill (2004) and Mitra \u0026amp; Reiter (2016) did not compare MI techniques to the other competing missing data technique of using GPS. All of the articles reviewed also generated the treatment indicator variable as a linear additive function of the covariates, estimated propensity scores using logistic regression, and estimated the treatment effect using multiple linear regression or just taking the plain difference in the means of the outcomes of the treated and untreated groups after matching or weighting. As I noted above, in applied settings, it is common to encounter large sets of covariates which may not have a linear additive relationship with the treatment assignment (Lee, Lessler, \u0026amp; Stuart, 2009). Using logistic regression to estimate propensity scores requires the functional form of the model to be specified correctly. Mis-specification can result in imbalanced covariates which can then result in biased treatment effect estimates.\nIn addition to reviewing the methodological studies above, I also conducted an analysis of 66 applied research articles using propensity scores across three journals focused on educational evaluation: Educational Evaluation and Policy Analysis (n = 38), American Educational Research Journal (n = 18), and Journal of Research on Educational Effectiveness (n = 10). I searched for the term “propensity score analysis” in the webpages of each of the journals and reviewed all of the articles that used propensity score analysis. The articles were from 2003 to 2019; I omitted one study published in the 1990s. Of these articles, 15% used complete case analysis, 14% used GPS, 36% used MI Within, 18% did not specify clearly how missing data were handled, and 17% used other techniques (e.g., hot-deck imputation, mean imputation without indicators). Three of the articles ran MI but only used one of the imputed datasets to conduct the analysis. None used MI Across even though Hill (2004) and Mitra \u0026amp; Reiter (2016) recommended it. Moreover, only one of these studies used machine learning algorithm, random forest, to estimate propensity scores (Im, Hughes, Cao, \u0026amp; Kwok, 2016). The examination of applied educational research using propensity scores and methodological studies on missing data methods for propensity score analysis poses the need to study missing data methods, especially GPS and MI, and how they perform with flexible propensity score estimation, especially in the context of educational research.\nComplete variables analysis is not a missing data method and can result in large bias in treatment effect estimates due to deletion of important confounding variables (Hill, 2004). DR is not readily available, has not been commonly used in applied literature, and has been shown to perform worse than MI (Hill, 2004). MIMP is also not widely used and has been shown to perform no better than GPS (Qu \u0026amp; Lipkovich, 2009).\n References Akande, O., Li, F., \u0026amp; Reiter, J. (2017). An empirical comparison of multiple imputation methods for categorical data. The American Statistician, 71(2), 162–170.\n Baraldi, A. N., \u0026amp; Enders, C. K. (2010). An introduction to modern missing data analyses. Journal of School Psychology, 48(1), 5–37. https://doi.org/10.1016/j.jsp.2009.10.001\n Crowe, B. J., Lipkovich, I. A., \u0026amp; Wang, O. (2010). Comparison of several imputation methods for missing baseline data in propensity scores analysis of binary outcome. Pharmaceutical Statistics, 9(4), 269–279. https://doi.org/10.1002/pst.389\n D’Agostino, R. B., \u0026amp; Rubin, D. B. (2000). Estimating and Using Propensity Scores with Partially Missing Data. Journal of the American Statistical Association, 95(451), 749. https://doi.org/10.2307/2669455\n Frangakis, C., \u0026amp; Rubin, D. B. (1999). Addressing complications of intention-to-treat analysis in the combined presence of all-or-none treatment-noncompliance and subsequent missing outcomes. Biometrika, 86(2), 365–379. https://doi.org/10.1093/biomet/86.2.365\n Gerber, A. S., \u0026amp; Green, D. P. (2012). Field experiments: Design, analysis, and interpretation (1st ed). New York: W. W. Norton.\n Hill, J. (2004). Reducing bias in treatment effect estimation in observational studies suffering from missing data. Columbia University Institute for Social \u0026amp; Economic Research \u0026amp; Policy (ISERP).\n Im, M. H., Hughes, J. N., Cao, Q., \u0026amp; Kwok, O.-m. (2016). Effects of extracurricular participation during middle school on academic motivation and achievement at grade 9. American Educational Research Journal, 53(5), 1343–1375.\n Lee, B. K., Lessler, J., \u0026amp; Stuart, E. A. (2009). Improving propensity score weighting using machine learning. Statistics in Medicine, n/a–n/a. https://doi.org/10.1002/sim.3782\n Leyrat, C., Seaman, S. R., White, I. R., Douglas, I., Smeeth, L., Kim, J., … Williamson, E. J. (2019). Propensity score analysis with partially observed covariates: How should multiple imputation be used? Statistical Methods in Medical Research, 28(1), 3–19. https://doi.org/10.1177/0962280217713032\n Little, R. J. (2008). Selection and pattern-mixture models. Longitudinal Data Analysis, 409–431.\n Mitra, R., \u0026amp; Reiter, J. P. (2016). A comparison of two methods of estimating propensity scores after multiple imputation. Statistical Methods in Medical Research, 25(1), 188–204. https://doi.org/10.1177/0962280212445945\n Mohan, K., Pearl, J., \u0026amp; Tian, J. (2013). Graphical models for inference with missing data. In C. Burges, L. Bottou, M. Welling, Z. Ghahramani, \u0026amp; K. Q. Weinberger (Eds.), Advances in neural information processing system (pp. 1277–1285). Red Hook, NY: Curran Associates, Inc.\n Murray, J. S. (2018). Multiple Imputation: A Review of Practical and Theoretical Findings. Statistical Science, 33(2), 142–159. https://doi.org/10.1214/18-STS644\n Qu, Y., \u0026amp; Lipkovich, I. (2009). Propensity score estimation with missing values using a multiple imputation missingness pattern (MIMP) approach. Statistics in Medicine, 28(9), 1402–1414. https://doi.org/10.1002/sim.3549\n Rosenbaum, P. R., \u0026amp; Rubin, D. B. (1983). The Central Role of the Propensity Score in Observational Studies for Causal Effects. Biometrika, 70(1), 41. https://doi.org/10.2307/2335942\n Rosenbaum, P. R., \u0026amp; Rubin, D. B. (1984). Reducing Bias in Observational Studies Using Subclassification on the Propensity Score. Journal of the American Statistical Association, 79(387), 516. https://doi.org/10.2307/2288398\n Rubin, D. B. (1976). Inference and Missing Data. Biometrika, 63(3), 581. https://doi.org/10.2307/2335739\n Rubin, D. B. (1987). Multiple imputation for nonresponse in surveys. New York: Wiley.\n Schafer, J. L., \u0026amp; Kang, J. (2008). Average causal effects from nonrandomized studies: A practical guide and simulated example. Psychological Methods, 13(4), 279–313. https://doi.org/10.1037/a0014268\n Stuart, E. A. (2010). Matching Methods for Causal Inference: A Review and a Look Forward. Statistical Science, 25(1), 1–21. https://doi.org/10.1214/09-STS313\n Thoemmes, F., \u0026amp; Mohan, K. (2015). Graphical Representation of Missing Data Problems. Structural Equation Modeling: A Multidisciplinary Journal, 22(4), 631–642. https://doi.org/10.1080/10705511.2014.937378\n van Buuren, S. (2018). Flexible imputation of missing data. Chapman; Hall/CRC.\n van Buuren, S., \u0026amp; Groothuis-Oudshoorn, K. (2011). mice: Multivariate imputation by chained equations in r. Journal of Statistical Software, 45(3), 1–67. Retrieved from http://www.jstatsoft.org/v45/i03/\n   ","date":1586908800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586908800,"objectID":"032142676fa4f11cdb208c2bf55e511d","permalink":"/post/missing_dat/","publishdate":"2020-04-15T00:00:00Z","relpermalink":"/post/missing_dat/","section":"post","summary":"Theories behind propensity score analysis assume that the covariates are fully observed (Rosenbaum \u0026amp; Rubin, 1983, 1984). However, in practice, observational analyses require large administrative databases or surveys, which inevitably will have missingness in the covariates. The response patterns of people with missing covariates may be different than those of people with observed data (Mohan, Pearl, \u0026amp; Tian, 2013). Therefore, ways to handle missing covariate data need to be examined.","tags":["propensity score","missing data","causal inference"],"title":"Missing Data in Propensity Score Analysis","type":"post"},{"authors":null,"categories":["Project"],"content":" Here is the website for the package.\nSimulations are experiments designed to study the performance of statistical methods under known data-generating conditions (Morris, White \u0026amp; Crowther, 2018). Methodologists examine questions like: (1) how does ordinary least squares (OLS) regression perform if errors are heteroskedastic? (2) how does the presence of missing data affect treatment effect estimates from a propensity score analysis? (3) how does cluster robust variance estimation perform when the number of clusters is small? To answer such questions, we conduct experiments by simulating thousands of datasets from pseudo-random sampling (Morris et al., 2018).\nThe goal of simhelpers is to assist in running simulation studies. This package provides a set of functions that calculates various performance measures like bias, root mean squared error, rejection rates, and also calculates the associated Monte Carlo standard errors (MCSE). These functions are divided into three major categories of performance criteria: absolute criteria, relative criteria, and criteria to evaluate hypothesis testing. The functions are created to work with the tidyeval practice.\nIn addition to the set of functions that calculates performance measures and MCSE, the package also includes a function, create_skeleton(), that generates a skeleton outline of a simulation study. Another function, evaluate_by_row(), runs the simulation for each combination of conditions row by row and implements the future_pmap() function from the furrr package to run the simulation in parallel. The package also contains several datasets that contain results from example simulation studies.\nInstallation You can install the development version from GitHub with:\n# install.packages(\u0026quot;devtools\u0026quot;) devtools::install_github(\u0026quot;meghapsimatrix/simhelpers\u0026quot;)  ","date":1583280000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583280000,"objectID":"bfaa4d5b29ba6edc1b222248de00df3f","permalink":"/project/internal-project/simhelpers/","publishdate":"2020-03-04T00:00:00Z","relpermalink":"/project/internal-project/simhelpers/","section":"project","summary":"Here is the website for the package.\nSimulations are experiments designed to study the performance of statistical methods under known data-generating conditions (Morris, White \u0026amp; Crowther, 2018). Methodologists examine questions like: (1) how does ordinary least squares (OLS) regression perform if errors are heteroskedastic? (2) how does the presence of missing data affect treatment effect estimates from a propensity score analysis? (3) how does cluster robust variance estimation perform when the number of clusters is small?","tags":["package","R"],"title":"simhelpers","type":"project"},{"authors":null,"categories":["R"],"content":"    I wanted to analyze the data from the April 2015 Nepal earthquake that resulted in around 10,000 deaths. I am using a dataset that I found in data.world. The data contains date, time, location and magnitude of the earthquake and the many aftershocks that followed. The data is updated as of June 2, 2015.\nNepal is my birthplace, my homeland. The earthquake was an extremely traumatic event for people who live there. Many people lost family members, their houses. I visited Nepal in 2017 and saw that every other house in Patan, Nepal (close to Kathmandu) was damaged. My relatives would talk about their experience of the earthquakes every day.\nLibraries library(tidyverse) library(geojsonio) library(broom) library(gganimate) library(leaflet) library(widgetframe)  Read in the data and clean earthquake_dat \u0026lt;- read_csv(\u0026quot;https://raw.githubusercontent.com/meghapsimatrix/Data_Visualization/master/data/earthquake-0-csv-1.csv\u0026quot;) %\u0026gt;% mutate(lab = paste0(Epicentre, \u0026quot;; \u0026quot;, Date,\u0026quot;; Magnitude(ML): \u0026quot;, `Magnitude(ML)`)) head(earthquake_dat) ## # A tibble: 6 x 7 ## Date `Local Time` Latitude Longitude `Magnitude(ML)` Epicentre lab ## \u0026lt;date\u0026gt; \u0026lt;time\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 2015-06-01 04:35 28.0 85.5 4 Sindhupal… Sindhup… ## 2 2015-05-31 13:54 28.3 84.5 4.2 Lamjung Lamjung… ## 3 2015-05-30 22:13 27.8 85.2 4.5 Nuwakot Nuwakot… ## 4 2015-05-30 20:35 28.0 85.2 4 Rasuwa/Nu… Rasuwa/… ## 5 2015-05-30 01:52 27.8 85.2 4 Dhading /… Dhading… ## 6 2015-05-29 15:44 28 85.0 5.2 Dhading Dhading… # there is one entry where I think the lat and long are switched summary(earthquake_dat$Latitude) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 26.94 27.71 27.82 28.06 27.98 84.71 summary(earthquake_dat$Longitude) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 28.16 85.23 85.80 85.25 86.06 86.67 # Gorkha seems like the lat and long are switched (outlier \u0026lt;- earthquake_dat %\u0026gt;% filter(Latitude == max(Latitude))) ## # A tibble: 1 x 7 ## Date `Local Time` Latitude Longitude `Magnitude(ML)` Epicentre lab ## \u0026lt;date\u0026gt; \u0026lt;time\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 2015-04-25 18:29 84.7 28.2 5.5 Gorkha Gorkha; … earthquake_dat \u0026lt;- earthquake_dat %\u0026gt;% mutate(Latitude = if_else(lab == outlier$lab \u0026amp; Date == outlier$Date, outlier$Longitude, Latitude), Longitude = if_else(lab == outlier$lab \u0026amp; Date == outlier$Date, outlier$Latitude, Longitude)) # Sindhupalchowk seems like the Longitude is wrong earthquake_dat %\u0026gt;% filter(Longitude == min(Longitude)) ## # A tibble: 1 x 7 ## Date `Local Time` Latitude Longitude `Magnitude(ML)` Epicentre lab ## \u0026lt;date\u0026gt; \u0026lt;time\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 2015-05-08 08:19 27.8 28.9 4.2 Sindhupal… Sindhup… sindhupalchowk \u0026lt;- earthquake_dat %\u0026gt;% filter(str_detect(Epicentre, \u0026quot;Sindhu\u0026quot;)) # Mean imputing based on other values for Sindhupalchowk earthquake_dat \u0026lt;- earthquake_dat %\u0026gt;% mutate(Longitude = if_else(Longitude == min(Longitude), mean(sindhupalchowk$Longitude), Longitude))  Make a Map of Nepal I got the code for the base map from here.\nnp \u0026lt;- geojson_read(\u0026quot;https://raw.githubusercontent.com/mesaugat/geoJSON-Nepal/master/nepal-districts.geojson\u0026quot;, what = \u0026quot;sp\u0026quot;) np_dat \u0026lt;- tidy(np) # plot np_plot \u0026lt;- ggplot() + geom_polygon(data = np_dat, aes( x = long, y = lat, group = group)) np_plot  Mapping on the Earthquake and Aftershocks Now plotting the latitude and longitudes. Size indicates the magnitude of the earthquake.\n(np_earthquake \u0026lt;- np_plot + geom_point(data = earthquake_dat, aes(x = Longitude, y = Latitude, size = `Magnitude(ML)`), color = \u0026quot;red\u0026quot;, alpha = .5) + labs(color = \u0026quot;\u0026quot;) + theme_void() + theme(legend.position = \u0026quot;none\u0026quot;))  Animating np_animate \u0026lt;- np_earthquake + transition_states(Date) + labs(title = \u0026#39;Date: {closest_state}\u0026#39;) + enter_appear() + exit_disappear() animate(np_animate)  Leaflet Created using the leaflet package. Click on the dots on the map to learn the location, date, and the magnitude of the earthquake or aftershock.\nnp_leaf \u0026lt;- leaflet(earthquake_dat) %\u0026gt;% setView(lat = 27, lng = 85, zoom = 7) %\u0026gt;% addProviderTiles(providers$CartoDB.DarkMatter) %\u0026gt;% addCircleMarkers(~Longitude, ~Latitude, radius = ~`Magnitude(ML)`, fillOpacity = 0.5, popup = ~lab, stroke = FALSE) frameWidget(np_leaf)  {\"x\":{\"url\":\"/post/Nepal_Earthquake_files/figure-html//widgets/widget_unnamed-chunk-6.html\",\"options\":{\"xdomain\":\"*\",\"allowfullscreen\":false,\"lazyload\":false}},\"evals\":[],\"jsHooks\":[]}  ","date":1574208000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1574208000,"objectID":"e3696d375d0ac0251c90bf6369ed4864","permalink":"/post/nepal_earthquake/","publishdate":"2019-11-20T00:00:00Z","relpermalink":"/post/nepal_earthquake/","section":"post","summary":"I wanted to analyze the data from the April 2015 Nepal earthquake that resulted in around 10,000 deaths. I am using a dataset that I found in data.world. The data contains date, time, location and magnitude of the earthquake and the many aftershocks that followed. The data is updated as of June 2, 2015.\nNepal is my birthplace, my homeland. The earthquake was an extremely traumatic event for people who live there.","tags":["Nepal","Nepal Earthquake 2015","gganimate","ggplot","leaflet"],"title":"Nepal Earthquake","type":"post"},{"authors":null,"categories":["R"],"content":"  Load the Data and Check Duplicates library(tidyverse) library(lubridate) library(kableExtra) library(ggridges) # there were complete duplicated rows dat \u0026lt;- read_csv(\u0026quot;https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-10-22/horror_movies.csv\u0026quot;) %\u0026gt;% distinct(.) # removes complete dups # check duplicates dup_title \u0026lt;- dat %\u0026gt;% filter(duplicated(title) | duplicated(title, fromLast = TRUE)) %\u0026gt;% arrange(title) # examined they seem different movies even though same title dup_title %\u0026gt;% filter(duplicated(plot)) ## # A tibble: 0 x 12 ## # … with 12 variables: title \u0026lt;chr\u0026gt;, genres \u0026lt;chr\u0026gt;, release_date \u0026lt;chr\u0026gt;, ## # release_country \u0026lt;chr\u0026gt;, movie_rating \u0026lt;chr\u0026gt;, review_rating \u0026lt;dbl\u0026gt;, ## # movie_run_time \u0026lt;chr\u0026gt;, plot \u0026lt;chr\u0026gt;, cast \u0026lt;chr\u0026gt;, language \u0026lt;chr\u0026gt;, ## # filming_locations \u0026lt;chr\u0026gt;, budget \u0026lt;chr\u0026gt; dup_title %\u0026gt;% filter(duplicated(release_date)| duplicated(release_date, fromLast = TRUE)) ## # A tibble: 2 x 12 ## title genres release_date release_country movie_rating review_rating ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; ## 1 The … Comed… 21-Jul-15 USA \u0026lt;NA\u0026gt; 5.2 ## 2 The … Comed… 21-Jul-15 USA NOT RATED 3.6 ## # … with 6 more variables: movie_run_time \u0026lt;chr\u0026gt;, plot \u0026lt;chr\u0026gt;, cast \u0026lt;chr\u0026gt;, ## # language \u0026lt;chr\u0026gt;, filming_locations \u0026lt;chr\u0026gt;, budget \u0026lt;chr\u0026gt; # The Jokesters seems to be a duplicate but with different rating and run time # Deleting it for now dat \u0026lt;- dat %\u0026gt;% filter(title != \u0026quot;The Jokesters (2015)\u0026quot;)  Genres The genre column looked extremely messy so some data munging fun. Each film can be categorized into multiple genres.\ndat_long \u0026lt;- dat %\u0026gt;% separate_rows(genres, sep = \u0026quot;\\\\|\u0026quot;) %\u0026gt;% # long format mutate(genres = str_trim(genres)) # Just to check - looks okay - just 1 movie with no genre table(dat_long$genres, useNA = \u0026quot;ifany\u0026quot;) ## ## Action Adult Adventure Animation Biography Comedy ## 335 1 115 39 4 511 ## Crime Drama Family Fantasy History Horror ## 120 529 11 229 6 3309 ## Music Musical Mystery Reality-TV Romance Sci-Fi ## 5 13 453 1 99 308 ## Sport Thriller War Western \u0026lt;NA\u0026gt; ## 4 1369 14 15 1 dat_long \u0026lt;- dat_long %\u0026gt;% mutate(genres = fct_infreq(fct_lump(genres, n = 8))) # Factor keeping 8 most frequent categories and lumping the rest to Other and order the factor by frequency Table: Number of Films per Genre genre_count \u0026lt;- dat_long %\u0026gt;% filter(!is.na(genres)) %\u0026gt;% group_by(genres) %\u0026gt;% summarize(n = n()) %\u0026gt;% ungroup() kable(genre_count, format = \u0026quot;html\u0026quot;, table.attr = \u0026quot;style = \\\u0026quot;color: white;\\\u0026quot;\u0026quot;) %\u0026gt;% kable_styling(bootstrap_options = \u0026quot;striped\u0026quot;, full_width = F)   genres  n      Horror  3309    Thriller  1369    Drama  529    Comedy  511    Mystery  453    Other  447    Action  335    Sci-Fi  308    Fantasy  229      Bar Graph: Distribution of Genres genre_count %\u0026gt;% ggplot(aes(x = genres, y = n, fill = genres)) + geom_bar(stat = \u0026quot;identity\u0026quot;) + scale_y_continuous(labels = scales::comma) + # y axis to have commas scale_fill_brewer(palette =\u0026quot;BuPu\u0026quot;, direction = -1) + # reverse order the palette theme_light() + labs(x = \u0026quot;\u0026quot;, y = \u0026quot;Number of Films\u0026quot;) + theme(legend.position = \u0026quot;none\u0026quot;)   Review Rating by Release Year Some of the years are dmy format, some just have the years. I am extracting the year and filling in any that didn’t parse with the year value from the original release_date column. No missing values for year :)\ndate_dat \u0026lt;- dat %\u0026gt;% mutate(date = dmy(release_date), yr = year(date), yr = ifelse(is.na(yr), release_date, yr)) table(is.na(date_dat$yr)) ## ## FALSE ## 3310 table(is.na(date_dat$review_rating)) ## ## FALSE TRUE ## 3058 252 date_dat %\u0026gt;% select(release_date, date, yr) %\u0026gt;% filter(is.na(date)) %\u0026gt;% head() ## # A tibble: 6 x 3 ## release_date date yr ## \u0026lt;chr\u0026gt; \u0026lt;date\u0026gt; \u0026lt;chr\u0026gt; ## 1 2017 NA 2017 ## 2 2013 NA 2013 ## 3 2012 NA 2012 ## 4 2013 NA 2013 ## 5 2017 NA 2017 ## 6 2017 NA 2017 date_dat %\u0026gt;% ggplot(aes(x = yr, y = review_rating, fill = yr)) + geom_boxplot(alpha = .5) + labs(x = \u0026quot;Release Year\u0026quot;, y = \u0026quot;Review Rating\u0026quot;) + theme_light() + theme(legend.position = \u0026quot;none\u0026quot;) Looks like there is a slight increase in ratings for newer films.\nAnd here is a ridgeline plot :)\ndate_dat %\u0026gt;% ggplot(aes(y = yr, x = review_rating, fill = yr)) + geom_density_ridges(alpha = .5) + labs(y = \u0026quot;Release Year\u0026quot;, x = \u0026quot;Review Rating\u0026quot;) + theme_light() + theme(legend.position = \u0026quot;none\u0026quot;)  ","date":1571616000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1571616000,"objectID":"02aac3f369bab187be802f517dd2fed9","permalink":"/post/tidy_tues_horror/","publishdate":"2019-10-21T00:00:00Z","relpermalink":"/post/tidy_tues_horror/","section":"post","summary":"Load the Data and Check Duplicates library(tidyverse) library(lubridate) library(kableExtra) library(ggridges) # there were complete duplicated rows dat \u0026lt;- read_csv(\u0026quot;https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-10-22/horror_movies.csv\u0026quot;) %\u0026gt;% distinct(.) # removes complete dups # check duplicates dup_title \u0026lt;- dat %\u0026gt;% filter(duplicated(title) | duplicated(title, fromLast = TRUE)) %\u0026gt;% arrange(title) # examined they seem different movies even though same title dup_title %\u0026gt;% filter(duplicated(plot)) ## # A tibble: 0 x 12 ## # … with 12 variables: title \u0026lt;chr\u0026gt;, genres \u0026lt;chr\u0026gt;, release_date \u0026lt;chr\u0026gt;, ## # release_country \u0026lt;chr\u0026gt;, movie_rating \u0026lt;chr\u0026gt;, review_rating \u0026lt;dbl\u0026gt;, ## # movie_run_time \u0026lt;chr\u0026gt;, plot \u0026lt;chr\u0026gt;, cast \u0026lt;chr\u0026gt;, language \u0026lt;chr\u0026gt;, ## # filming_locations \u0026lt;chr\u0026gt;, budget \u0026lt;chr\u0026gt; dup_title %\u0026gt;% filter(duplicated(release_date)| duplicated(release_date, fromLast = TRUE)) ## # A tibble: 2 x 12 ## title genres release_date release_country movie_rating review_rating ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; ## 1 The … Comed… 21-Jul-15 USA \u0026lt;NA\u0026gt; 5.","tags":["tidy tuesday","ggplot","tidyverse"],"title":"Tidy Tuesday Horror","type":"post"},{"authors":null,"categories":["R"],"content":" In my qualifying exam, in the written part, I was asked about how to analyze the effect of continuous, not binary, treatment using propensity score analysis. I skipped it for the written but I spent a few days looking up how to analyze this in case I would be asked during my oral examination. Sadly, no one asked me even when I asked them to, so here is a blog detailing my explorations.\nBinary Treatment For a review of propensity score analysis with binary treatment, please see Stuart (2010). Below let \\(e(X)\\) denote propensity scores, \\(D\\) denote a binary treatment, and \\(X\\) denote all the observed confounders. In the case of binary treatment, propensity scores represent the probability of receiving treatment given the covariates:\n\\[e(X) = P(D = 1|X)\\]\nWe estimate the scores using logistic regression or machine learning techniques like generalized boosted models.\n Extension to Contiuous Treatment In binary treatment context, we assume that the potential outcomes (\\(Y(1)\\) and \\(Y(0)\\)) are independent of treatment given \\(X\\):\n\\[Y(1), Y(0) \\perp\\!\\!\\!\\perp D |X\\]\nand by extension are independent given the propensity scores:\n\\[Y(0), Y(1) \\perp\\!\\!\\!\\perp D|e(X)\\]\nHirano and Imbens (2004) introduced the assumption of weak unconfoundedness in the context of continuous treatment. They stated: “we do not require joint independence of all potential outcomes. Instead, we require conditional independence to hold for each value of the treatment.” Below, let \\(T\\) denote a continuous treatment variable. The potential outcome when \\(T = t\\) is unreated to the treatment given the set of covariates:\n\\[Y(t) \\perp\\!\\!\\!\\perp T |X\\]\nTo calculate the propensity scores, in the case of continuous treatment, we cannot find the probability that continuous treatment (\\(T\\)) equals a given value \\(t\\). The likelihood of continuous variables taking on a given value is zero. For continuous treatment variable, we find the conditional density, the probability that \\(T\\) is infinitely close to \\(t\\) given \\(X\\). Below let \\(r(t,x)\\) denote the propensity scores. The right hand side of the equation represents the probability density function of a normal distribution. To estimate the propensity scores, we need to run a linear regression predicting the treatment by a set of covariates (Austin, 2019). From that we get the fitted values (\\(X\\hat{\\beta}\\)) and the model variance (\\({\\sigma}^2\\)) (Austin, 2019). The fitted values take the place of the mean in the density function.\n\\[ r(t, x) = {f_{T|X}}^{(t|x)} = \\frac{1}{\\sqrt{2\\pi\\hat{\\sigma}^2}} e^{-\\frac{(t - X\\hat{\\beta})^2}{2\\pi\\hat{\\sigma}^2}}\\]\nConditional on the propensity scores, we can assume that each potential outcome is independent of treatment:\n\\[Y(t) \\perp\\!\\!\\!\\perp T |r(t,x)\\]\nHirano and Imbens (2004) state that: “Within strata with the same value of \\(r(t,X)\\), the probability that \\(T = t\\) does not depend on the value of \\(X\\).” I have seen \\(1\\) and \\(I\\) in front of the \\((T = t)\\), denoting the indicator function (Hirano \u0026amp; Imbens, 2004; Bia \u0026amp; Mattei, 2008).\n\\[X \\perp\\!\\!\\!\\perp 1(T = t)|r(t,x)\\]\n Calculating Weights Following the same logic as the inverse propensity weights (IPW) for the estimation of the average treatment effect (ATE) for a binary treatment, we calculate the inverse of the propensity scores as the weights:\n\\[\\frac{1}{{f_{T|X}}^{(t|x)}}\\]\nHowever, Robins et al. (2000) noted that such weights can result in infinite variance (Austin, 2019). They suggested to use stabilized weights as follows:\n\\[\\frac{{f_{T}}^{(t)}}{{f_{T|X}}^{(t|x)}}\\]\nHere the numerator represents the marginal density of treatment:\n\\[{f_{T}}^{(t)} = \\frac{1}{\\sqrt{2\\pi\\hat{\\sigma_t}^2}} e^{-\\frac{(t - \\mu_t)^2}{2\\pi\\hat{\\sigma_t}^2}}\\]\nThe stabilized weights make the distribution of the IPW narrower as there is less difference between the numerators and the denominators (van der Wal \u0026amp; Geskus, 2011).\n Real Data Analysis Example The data that I use here is from High School and Beyond (HSB) longitudinal study used by Rosenbaum (1986) to analyze the effect of dropping out of high school on later math achievement. The missing data in the original dataset have been replaced with one iteration of imputation using mice (van Buuren \u0026amp; Groothuis-Oudshoorn, 2011). This is not an appropriate method to analyze missing data but for the purpose of the example I am just using the one complete data. For the sake of this example, let’s analyze the effect of math efficacy on later math achievement.\nLoading the Data library(tidyverse) dat \u0026lt;- read_csv(\u0026quot;https://raw.githubusercontent.com/meghapsimatrix/R_practice/master/Data/HSLS09_complete.csv\u0026quot;)  The Numerators Here I am getting the numerators of the IPW, the marginal densities. I have regressed math_efficacy on just the intercept and used dnorm function to extract the densities.\n# the numerator mod_num \u0026lt;- lm(math_efficacy ~ 1, data = dat) num \u0026lt;- dnorm(x = dat$math_efficacy, # treatment mean = fitted.values(mod_num), # fitted values sd = summary(mod_num)$sigma) # model sigma  The Denominators Here I am getting the denominators of the IPW, the conditional densities. I have regressed math_efficacy on \\(X\\) and used dnorm function to extract the densities. I am not quite sure whether to use the model sigma which divides the sum of errors squared by the degrees of freedom before taking the square root or whether I should just take the standard deviation of the errors. However, with large sample size the difference between the two are negligible.\n# the demonimator mod_den \u0026lt;- lm(math_efficacy ~ sex + race + language + repeated_grade + IEP + locale + region + SES, data = dat) den \u0026lt;- dnorm(x = dat$math_efficacy, # treatment variable mean = fitted.values(mod_den), # fitted values sd = summary(mod_den)$sigma)  The IPW Below I calculate the stabilized weights:\ndat \u0026lt;- dat %\u0026gt;% mutate(ipw_s = num/den) summary(dat$ipw_s) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.1398 0.9186 0.9778 1.0001 1.0390 5.9782  Checking Balance and Outcome Analysis Please check back on my next post :) Short story: For balance, we have to calculate weighted correlations, and for outcome analysis we estimate the expected outcome for each treatment level and compare (Austin, 2019).\n  References Bia, M., \u0026amp; Mattei, A. (2008). A Stata package for the estimation of the dose-response function through adjustment for the generalized propensity score. The Stata Journal, 8(3), 354-373.\nHirano K and Imbens GW. The propensity score with continuous treatments. In: Gelman A and Meng X-L (eds) Applied Bayesian modeling and causal inference from incomplete-data perspectives. Chichester: John Wiley \u0026amp; Sons Ltd, 2004, pp.73–84.\nRobins JM, Hernan MA and Brumback B. Marginal structural models and causal inference in epidemiology. Epidemiol 2000; 11: 550–560.\nRosenbaum, P. R. (1986). Dropping out of high school in the United States: An observational study. Journal of Educational Statistics, 11(3), 207-224.\nStuart, E. A. (2010). Matching methods for causal inference: A review and a look forward. Statistical science: a review journal of the Institute of Mathematical Statistics, 25(1), 1.\nvan Buuren, S., \u0026amp; Groothuis-Oudshoorn, K. (2011). mice: Multivariate imputation by chained equations in r. Journal of Statistical Software, 45 (3), 1–67. Retrieved from http://www.jstatsoft.org/v45/i03/\nvan der Wal, W. M., \u0026amp; Geskus, R. B. (2011). Ipw: an R package for inverse probability weighting. J Stat Softw, 43(13), 1-23.\nZhu, Y., Coffman, D. L., \u0026amp; Ghosh, D. (2015). A boosting algorithm for estimating generalized propensity scores with continuous treatments. Journal of causal inference, 3(1), 25-40.\n ","date":1570579200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1570579200,"objectID":"5923748eee57f4ce57894287497ab840","permalink":"/post/continuous-r-rmarkdown/","publishdate":"2019-10-09T00:00:00Z","relpermalink":"/post/continuous-r-rmarkdown/","section":"post","summary":"In my qualifying exam, in the written part, I was asked about how to analyze the effect of continuous, not binary, treatment using propensity score analysis. I skipped it for the written but I spent a few days looking up how to analyze this in case I would be asked during my oral examination. Sadly, no one asked me even when I asked them to, so here is a blog detailing my explorations.","tags":["propensity score analysis","causal inference","continuous treatment"],"title":"Continuous Treatment in Propensity Score Analysis","type":"post"},{"authors":[],"categories":[],"content":" Welcome to Slides Academic\nFeatures  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides  Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E  Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)  Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = \\;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \nA fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears  Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view   Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links   night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links  Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/img/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}  Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }  Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Academic's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":["Megha Joshi, Melissa L Aikens, Erin L Dolan"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math.\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"b96dcfe9f79c6bde909ed6fa02b8d38d","permalink":"/publication/mentoring/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/publication/mentoring/","section":"publication","summary":"Direct ties to faculty related to better outcomes for undergraduate life-science researchers.","tags":["structural equation modeling","mentoring structures","undergraduate research"],"title":"Direct Ties to a Faculty Mentor Related to Positive Outcomes for Undergraduate Researchers","type":"publication"},{"authors":["Keenan A Pituch, Megha Joshi, Molly E Cain, Tiffany A Whittaker, Wanchen Chang, Ryoungsun Park, Graham J McDougall"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math.\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"fc4382c56f4947d13534da98a7570ca7","permalink":"/publication/missingdata/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/publication/missingdata/","section":"publication","summary":"Missing data treatment using multivariate methods for two-group comparisons and small sample sizes.","tags":["missing data","multivariate analysis","multilevel modeling","maximum likelihood"],"title":"The Performance of Multivariate Methods for Two-Group Comparisons with Small Samples and Incomplete Data","type":"publication"}]